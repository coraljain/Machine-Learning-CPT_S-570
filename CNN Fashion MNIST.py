#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# Import
import os # for file handling
import pandas as pd # for data handling
import numpy as np # for linear algebra
import time # to time runs


# In[ ]:


import matplotlib.pyplot as plt # to display images
from sklearn import metrics # to evaluate classification accuracy
import tensorflow as tf # for neural networks


# In[ ]:


from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers import BatchNormalization, Dropout


# In[ ]:


get fashion mnist data
(x_train,y_train), (x_test,y_test) = tf.keras.datasets.fashion_mnist.load_data()

# show shapes of tensors
print("x_train shape:", x_train.shape, ", y_train shape:", y_train.shape)
print("x_test shape:", x_test.shape, ", y_test shape:", y_test.shape)

# get number of classes
nClasses = len(np.unique(y_train)) # number of output classes
print("Number of classes: ", nClasses)


# In[ ]:


# normalize grayscale pixel values (0-255) to (0,1)
x_train = x_train.astype('float32')/255 # normalized training inputs
x_test = x_test.astype('float32')/255 # normalized test inputs

# reshape to needed input shape for network
x_train, x_test = x_train.reshape((-1,28,28,1)), x_test.reshape((-1,28,28,1))
input_shape = x_train.shape[1:] # input shape for network

# show shapes of re-shaped tensors
print("x_train shape:", x_train.shape, ", y_train shape:", y_train.shape)
print("x_test shape:", x_test.shape, ", y_test shape:", y_test.shape)

# get image dimensions
img_h, img_w, img_channels = x_train.shape[1:] # size of image
print("Image height = %d, image width = %d, number of channels = %d" 
      %(img_h, img_w, img_channels))


# In[ ]:


{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** code below uses [PyTorch](https://pytorch.org/tutorials/) but the ideas are similar in most of the other deep learning tools ([TensorFlow](https://www.tensorflow.org/tutorials), [MXNet](http://d2l.ai/) ... etc.)**\n",
    "\n",
    "Text content (like this cell) is written in [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Highly recommended - FastAI course](https://course.fast.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor\n",
    "import torch\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train_data = pd.read_csv('fashionmnist/fashion-mnist_train.csv')\n",
    "    test_data = pd.read_csv('fashionmnist/fashion-mnist_test.csv')\n",
    "    x_train = train_data[train_data.columns[1:]].values\n",
    "    y_train = train_data.label.values\n",
    "    x_test = test_data[test_data.columns[1:]].values\n",
    "    y_test = test_data.label.values\n",
    "    return map(tensor, (x_train, y_train, x_test, y_test)) # maps are useful functions to know\n",
    "                                                           # here, we are just converting lists to pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = get_data()\n",
    "train_n, train_m = x_train.shape\n",
    "test_n, test_m = x_test.shape\n",
    "n_cls = y_train.max()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray' # it is good to try different ways to visualize your data \n",
    "                                    # matplotlib is a good library although its interface is pretty bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1091765f8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[torch.randint(train_n, (1,))].view(28, 28)) # visualize a random image in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](lenet.jpg \"lenet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the model\n",
    "class FashionMnistNet(nn.Module):\n",
    "    # Based on Lecunn's Lenet architecture\n",
    "    def __init__(self):\n",
    "        super(FashionMnistNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3) # convolution demo (http://cs231n.github.io/convolutional-networks/)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3) \n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the model\n",
    "# model = FashionMnistNet()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.forward(x_train.float().reshape(train_n, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Numerical Stability **\n",
    "- Remember to keep all your values float wherever required (you might get wrong/undesirable behavior with integer values). Try running the next two cells for an example and see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FashionMnistNet()\n",
    "# model.forward(x_train[0].reshape(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.forward(x_train[0].float().reshape(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question - Why shouldn't we initialize neural network weights to zero?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One good initialization (used as default in pytorch) is kaiming initialization\n",
    "\n",
    "- [Kaiming initialization of weights](https://arxiv.org/abs/1502.01852)\n",
    "\n",
    "- [Blogpost explaining kaiming](https://pouannes.github.io/blog/initialization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization is so important that you might not even require any normalization layer like batchnorm\n",
    "\n",
    "[See Fixup initialization](https://openreview.net/forum?id=H1gsz30cKX) (10,000 layers) **(°◇°)\t**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FashionMnistNet() # Creating a model\n",
    "lr = 0.05 # learning rate\n",
    "epochs = 10 # number of epochs\n",
    "bs = 32 # batch size \n",
    "loss_func = F.cross_entropy # loss function \n",
    "opt = optim.Adam(model.parameters(), lr=lr) # optimizer\n",
    "accuracy_vals = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Why shouldn't we use large batch sizes?](https://arxiv.org/abs/1609.04836)\n",
    "\n",
    "[There is an answer on stackoverflow that explains the same](https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  80.9800033569336\n",
      "Accuracy:  82.76000213623047\n",
      "Accuracy:  83.88999938964844\n",
      "Accuracy:  84.0\n",
      "Accuracy:  84.18000030517578\n",
      "Accuracy:  83.87999725341797\n",
      "Accuracy:  84.01000213623047\n",
      "Accuracy:  83.41000366210938\n",
      "Accuracy:  83.76000213623047\n",
      "Accuracy:  83.91999816894531\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    #print(model.training)\n",
    "    for i in range((train_n-1)//bs + 1): # (train_n-1)//bs equals the number of batches when we divide the divide by given batch size bs \n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        # Pytorch reshape function has four arguments -  (batchsize, number of channels, width, height)\n",
    "        xb = x_train[start_i:end_i].float().reshape(bs, 1, 28, 28) \n",
    "        yb = y_train[start_i:end_i]\n",
    "        loss = loss_func(model.forward(xb), yb) # model.forward(xb) computes the prediction of model on given input xb\n",
    "        loss.backward() # backpropagating the gradients\n",
    "        opt.step() # gradient descent \n",
    "        opt.zero_grad() # don't forget to add this line after each batch (zero out the gradients)\n",
    "        \n",
    "    model.eval()\n",
    "    #print(model.training)\n",
    "    with torch.no_grad(): # this line essentially tells pytorch don't compute the gradients for test case\n",
    "        total_loss, accuracy = 0., 0.\n",
    "        for i in range(test_n):\n",
    "            x = x_test[i].float().reshape(1, 1, 28, 28)\n",
    "            y = y_test[i]\n",
    "            pred = model.forward(x)\n",
    "            accuracy += (torch.argmax(pred) == y).float()\n",
    "        print(\"Accuracy: \", (accuracy*100/test_n).item())\n",
    "        accuracy_vals.append((accuracy*100/test_n).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11b9841d0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracy_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization is very important!\n",
    "[Why?](https://www.youtube.com/watch?v=FDCfw-YqWTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(72.9505), tensor(89.9669))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Normalization\n",
    "x_train, x_test = x_train.float(), x_test.float()\n",
    "train_mean,train_std = x_train.mean(),x_train.std()\n",
    "train_mean,train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, m, s): return (x-m)/s\n",
    "x_train = normalize(x_train, train_mean, train_std)\n",
    "x_test = normalize(x_test, train_mean, train_std) # note this normalize test data also with training mean and standard deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  83.45999908447266\n",
      "Accuracy:  86.3499984741211\n",
      "Accuracy:  87.5\n",
      "Accuracy:  87.87999725341797\n",
      "Accuracy:  88.30999755859375\n",
      "Accuracy:  89.01000213623047\n",
      "Accuracy:  89.13999938964844\n",
      "Accuracy:  89.45999908447266\n",
      "Accuracy:  89.4000015258789\n",
      "Accuracy:  89.54000091552734\n"
     ]
    }
   ],
   "source": [
    "model_wnd = FashionMnistNet()\n",
    "lr = 0.05 # learning rate\n",
    "epochs = 10 # number of epochs\n",
    "bs = 32\n",
    "loss_func = F.cross_entropy\n",
    "opt = optim.SGD(model_wnd.parameters(), lr=lr)\n",
    "accuracy_vals_wnd = []\n",
    "for epoch in range(epochs):\n",
    "    model_wnd.train()\n",
    "    for i in range((train_n-1)//bs + 1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xb = x_train[start_i:end_i].reshape(bs, 1, 28, 28)\n",
    "        yb = y_train[start_i:end_i]\n",
    "        loss = loss_func(model_wnd.forward(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    model_wnd.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss, accuracy = 0., 0.\n",
    "        validation_size = int(test_n/10)\n",
    "        for i in range(test_n):\n",
    "            x = x_test[i].reshape(1, 1, 28, 28)\n",
    "            y = y_test[i]\n",
    "            pred = model_wnd.forward(x)\n",
    "            accuracy += (torch.argmax(pred) == y).float()\n",
    "        print(\"Accuracy: \", (accuracy*100/test_n).item())\n",
    "        accuracy_vals_wnd.append((accuracy*100/test_n).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11bacb588>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAHmxJREFUeJzt3X141fV9//HnOzkJyQmBhBDuAgEEuRO5M1K0VadgdZUW8JqbdWrVWdetq5u/1W7dtc7dtN1+m792v7W9utJWO7U/OkWItlWL1rpZJ2jggNwEBURCTm4IARJIyM3Jef/+SFSwQiLm5HtuXo/r4iI553s8Lw/JK9+8z/f7/Zi7IyIiqS8r6AAiIjI4VOgiImlChS4ikiZU6CIiaUKFLiKSJlToIiJpQoUuIpImVOgiImlChS4ikiZCQ/lko0eP9ilTpgzlU4qIpLzNmzcfdvfS/rYb0kKfMmUKVVVVQ/mUIiIpz8wODGQ7jVxERNKECl1EJE2o0EVE0oQKXUQkTajQRUTShApdRCRNqNBFRNLEkB6HLiKSCWI9cepbOqg50k7NkXYONLfz+SunUZiXk9DnVaGLiJyDE50xaprbqTnSdlpxHzzSTu3Rk8Ti767XnJNtrFw4gVnjVOgiIkMuHncOHe/kQPO7hX1qaTe3dZ22/cj8HCaXhLmgbCSfuHA85aPClJeEKR8VZvzIfLKzLOGZVegiMihiPXGqDhzluV2NvPBGE52xHgqH5VCYF6Iw7+2/Q+98PnxY78cj+u4bfsp2w3NDZA1BAXZ093Cwr6RPLe2aI72l3RmLv7NtlsGEonwml4T5+AVjmTQqzORRBb3FPSrMyHBi974HQoUuIuesrTPGi3ua2LCrkV/tPsTR9m5ys7NYMq2EkoJcjnd009oRI3rsJCc6uzneEeN4R4yeU8YRZ/J24b/3B0BhXg4j8kKnff72D4QReaf/AAllGc1tXe/sVb9b3L173Y2tnac9Z0FuNuUlBUwrLeCqWWP6Sru3sMuK88nJTu7jSFToIvKBHGrt4LnqQzy7q4GX9jXTFYszMj+Hq2aN4eo5Y7l8RinDh525Wtydk909nOiI0doR43hHb9Gf6Hz349aOGCdOue94ZzfH2rs4eKT9ncecuvd8JqEsO22WDTBuRB7lJWEuO7+0t6xLwu8U96iCXMwS/5tBoqjQReSs3J09h07w7K5GNuxqZNvBYwBMGpXPzR+ZzNVzxnLxlGJCA9x7NTPCuSHCuSHGjDj3XF2x+Gk/BI6f+gOgo5sTnTHaunoYUziMyX2z7InFYfJyss/9SZOcCl1EfsPb8/BndzXyXHUjB5rbAZg/cSRf/PgMrp4zjhljhwe6N5sbymJUKJdRBbmBZUg2KnQRAXrn4f/9RhPPVjfy/O5DHOubh186vYS7Lj+PZbPHMnZEXtAx5SxU6CIZ7FBrB89WN/LcrsbT5uFLZ41h2QDm4ZJc9C8lkkHcnTcaT/Bc9enz8PJRYW5ZMpllsz/YPFySiwpdJM2dOg9/dlcjNUf65uGTipJmHi6DQ4UukoZOdMZ48Y0mnt3VyPOvnz4P/8MrNA9PVyp0kRTm7hxt76b2aO/1Qw4eaeflN5v5n73NdPXEKQrncNXM3uPDL9M8PO3pX1ckib23sN/9+92P27t6TntM+agwt1yieXgmGlChm9k9wJ2AA9uB24FLgfuBXGAz8AfuHktQTpG0dC6FPSIvxMTiMFNKCvjY9FImFuf3/ek9PX1kfvDXFJFg9FvoZlYG3A3McfeTZvYocBPwd8BSd3/DzP4e+Azww4SmFUkxKmwZSgMduYSAfDPrBsJAG9Dp7m/03f8s8GVU6JKhjrZ18fKbzSpsCVS/he7uUTO7H6gBTgIbgEeBfzazCnevAn4HmJTQpCJJ6mev1fGVyh0cbe8GVNgSnIGMXIqBFcBU4BjwGPD7wI3AN81sGL0l/77zczO7C7gLoLy8fHBSiySBI21dfOWJHfz8tXrmTxzJ92+dw/ljC1XYEpiBjFyWAfvdvQnAzNYBl7r7I8Blfbd9HJjxfg9299XAaoCKior+L4IskgKe2dHAX1dup+VkN/deM5M/vPw8HU0igRtIodcAS8wsTO/IZSlQZWZj3P1Q3x76XwBfS2BOkaRwrL2Lv31yJ5Vb67hgwggeufMjzBr3Ia4BKzKIBjJD32Rma4Et9I5VIvTucX/VzJYDWcB33f35hCYVCdgvqxv5y3XbOdrWxT3LZvDHV05L+hVsJLOY+9BNQSoqKryqqmrInk9kMLSc7Obvf7qLx7fUMmtcIfffMJ+5ZSODjiUZxMw2u3tFf9vpTFGRs3jh9UP85ePbaTrRyReums4Xrjqf3JD2yiU5qdBF3sfxjm6+/lQ1a145yPljhvO9Wy5i/qSioGOJnJUKXeQ9Xtp7mC+tfY36lpN87opp/Nmy89N6HUpJHyp0kT5tnTH+8elqHtlYw3mjC3jsc5dy0eTioGOJDJgKXQTY+GYz967dRu3Rk9z5sal88ZqZ2iuXlKNCl4x2squH//3Mbn70P28xuSTMo394CRdPGRV0LJFzokKXjFX11hG++Ng23mpu57ZLp/Cla2cSztW3hKQuffVKxuno7uH/bHidH/x6P2VF+az57BIumVYSdCyRD02FLhklUnOULz62jX1Nbfz+R8r58idma1k2SRv6SpaM0Bnr4V+f28P3/msf40bk8fAfLOay80uDjiUyqFTokva217bw549t5Y3GE9x48ST+6rrZjMjTJW4l/ajQJW11xeJ8+/k9fOeFfYwensuDt1/MlTPHBB1LJGFU6JKWdtW18uePbaO6vpXrF5Vx3/ILGBnWXrmkNxW6pJXunjjffWEf//bLPRQX5PL9Wyu4es7YoGOJDAkVuqSN1xuO88XHtrE92sKKBRP4209eQHFBbtCxRIaMCl1SXqwnzuoX3+Rfn91DYV6If795EdfOHR90LJEhp0KXlOXuvLL/CF9/ejfbDh7jExeO4x9WzKVk+LCgo4kEQoUuKed4RzeVkSgPbzzAG40nGFWQy7c+vZBPzp8QdDSRQKnQJWXsbmjl4ZcPUBmJ0tbVw7yJI/nn35nHJ+dNID9XV0YUUaFLUuuM9fDMjgYe2XiAV986yrBQFp+aP4Gbl0zWCkIi76FCl6RUe7Sd/7ephkerDnL4RBdTSsL89XWz+Z2LJlIU1pErIu9HhS5JIx53/ntPE49sPMDzuw8BsHT2WG5ZMpmPTR9NVpYFnFAkuanQJXBH27p4bPNBHtlYQ82RdkYPH8bnr5zOjYvLKSvKDzqeSMpQoUsg3J2tB4/x8MYD/Oy1erpicRZPHcW918zkmgvGkRvKCjqiSMoZUKGb2T3AnYAD24HbgY8C/wJkASeA29x9b4JySpo42dXDk9t6DzncEW1l+LAQv1cxiZuXTGbmuMKg44mktH4L3czKgLuBOe5+0sweBW4E/gpY4e7VZvbHwF8DtyUyrKSufU0neGTjAdZuruV4R4xZ4wr56sq5rFxYpgUmRAbJQL+TQkC+mXUDYaCO3r31EX33j+y7TeQdsZ44z1U38vDGA7y0t5mcbOO3547nlksmUzG5GDO9ySkymPotdHePmtn9QA1wEtjg7hvM7E7gKTM7CbQCS97v8WZ2F3AXQHl5+aAFl+TV2NrBmldq+MkrB2lo7aCsKJ97r5nJ71ZMorRQp+WLJMpARi7FwApgKnAMeMzMbgauBz7h7pvM7F7gG/TO2U/j7quB1QAVFRU+iNklibg7L7/ZzCMbD/CLnY30xJ0rZpTy1ZVzuXLWGLJ1yKFIwg1k5LIM2O/uTQBmto7eN0Tnu/umvm3+E3gmMRElmbV2dLNucy0PbzzAvqY2isI5/MHHpnLT4nKmjC4IOp5IRhlIodcAS8wsTO/IZSlQBdxgZjPc/Q3gaqA6cTElmbg7W2qOsXbzQSojdZzs7mHBpCLuv2E+y+eNJy9H11URCcJAZuibzGwtsAWIARF6Ryi1wONmFgeOAnckMqgEr6a5nfWRKOsjtbzV3E5eThYr5pdx85LJXDhxZNDxRDKeuQ/dWLuiosKrqqqG7Pnkw2s52c3PX6tnfaSWV986ihksmVrC9YvKuHbuOArztE6nSKKZ2WZ3r+hvOx0ALL+huyfOf73exLpILc9VH6IrFmf6mOHce81MVi4s0+n4IklKhS5A71x8e7SFdVuiPLmtjiNtXYwqyOWmxeVcv6iMC8tG6rhxkSSnQs9w0WMnqYxEWbelln1NbeSGsrh69liuX1TG5TNKycnWNVVEUoUKPQMd7+jm6R0NrN8SZeP+Ztxh8ZRR3HnZeXziwvGMzNdcXCQVqdAzRKwnzq/3HmbdligbdjXQ0R1nSkmYe5bNYNXCMiaNCgcdUUQ+JBV6GnN3dtW3sn5LlMqtdRw+0UlROIcbLprEqkVlLJxUpLm4SBpRoaehxtYOKiNR1kei7G44Tk62cdWsMaxaOJErZ5UyLKQTf0TSkQo9TbR3xfjFzgbWbYny0t7DxB0WlhfxDyvnsvzC8RQXaB1OkXSnQk9hPXHn5X3NrIvU8syOBtq7ephYnM+fXDmdVYsmMlXXUhHJKCr0FLT30Ake23yQJyJ1NLR2UJgXYsWCCaxaOJGKycVaTFkkQ6nQU8yvdh/isw/1Xj7hihmlfGX5HJbOHqMLYomICj2VbKk5yh/9eDOzx4/ggdsu1mIRInIaFXqK2HvoOHf86FXGjcjjwdsvZvRwlbmInE7ndaeAhpYObv3hK4Sysnjojo+ozEXkfanQk1xLezefeeAVWjti/McdF1NeojM6ReT9qdCTWEd3D3c+9Cr7D7ex+taLuGCCFpEQkTPTDD1JxXrifGFNhKoDR/n2pxdx6bTRQUcSkSSnPfQk5O585YkdPLurkb/71AVcN2980JFEJAWo0JPQN599gzWvHOQLV03n1kumBB1HRFKECj3JPPzyW/zb83u58eJJ/K+rZwQdR0RSiAo9iTy1vZ6/eXIny2aP5asr5+rStiLygajQk8TL+5r5s59s5aLyYr5900JCWvpNRD4gtUYS2FnXwl0PVTFldJgffKZC12URkXMyoEI3s3vMbKeZ7TCzNWaWZ2YvmtnWvj91ZlaZ6LDp6OCRdm578FUK80L8xx2LKQrruuUicm76PQ7dzMqAu4E57n7SzB4FbnT3y07Z5nHgicTFTE+HT3Ryyw830d0TZ81nL2H8yPygI4lIChvoyCUE5JtZCAgDdW/fYWaFwFWA9tA/gLbOGHf86FUaWjv44WcuZvqYwqAjiUiK67fQ3T0K3A/UAPVAi7tvOGWTVcAv3b01MRHTT1cszuce2czOula+c9MiLppcHHQkEUkD/Ra6mRUDK4CpwASgwMxuPmWTTwNrzvL4u8ysysyqmpqaPmzelBePO/eu3caLew7zT9dfyNLZY4OOJCJpYiAjl2XAfndvcvduYB1wKYCZlQCLgZ+f6cHuvtrdK9y9orS0dDAypyx352tPVfPE1jq+dO1MbqiYFHQkEUkjAyn0GmCJmYWt90yXpUB13303AD9z945EBUwnq//7TX746/3c/tEp/NEV04KOIyJpZiAz9E3AWmALsL3vMav77r6Rs4xb5F2Pb67lH5/ezSfnT+Ar183RWaAiMugGdPlcd78PuO99bv+twQ6Ujn61+xBfevw1PjZ9NPffMI+sLJW5iAw+nSmaYJGao/zxj7cwe3wh/37LRQwL6SxQEUkMFXoC7T10gjt+9CpjRgzjwdsWM3yY1hMRkcRRoSdIQ0sHn3ngFbKzjIfuWExpoRZ2FpHE0i5jAry9sHPLyW5+ctcSJpcUBB1JRDKA9tAHWUd3D599qIo3D59g9S0XMbdMCzuLyNDQHvogivXEuXtNhFcPHOFbn17IpdO1sLOIDB3toQ+S3oWdd7JhVyP3LZ/D8nkTgo4kIhlGhT5IvvncHta8UsPnr5zGbR+dGnQcEclAKvRB8PDGA/zbL/fwuxUT+eLHZwYdR0QylAr9Q3pqez1/88QOls4aw9dXXahT+kUkMCr0D+HthZ0XTiri2zct0sLOIhIoNdA52lXXyl0PVVFeEuaB2y4mP1en9ItIsFTo5+DgkXY+8+ArDM8L8ZAWdhaRJKFC/4CaT3Ry6wOv0BWL89Adi5lQpIWdRSQ5qNA/gLbOGLf/6FXqW07ywG0VnD9WCzuLSPLQmaIfwD3/uZWdda187+aLuGjyqKDjiIicRnvoA3SguY0Nuxr5wlXTWTZHCzuLSPJRoQ9QZaQOM/hdLewsIklKhT4A7k7l1ihLppboTVARSVoq9AHYVtvC/sNtrFpUFnQUEZEzUqEPwPottQwLZXHt3HFBRxEROSMVej+6e+L89LV6ls0Zy4i8nKDjiIickQq9Hy/uaeJIWxerFmjcIiLJTYXej/WROorDOVw+ozToKCIiZzWgQjeze8xsp5ntMLM1ZpZnvb5mZm+YWbWZ3Z3osEPteEc3G3Y2sHzeBHJD+tknIsmt3zNFzawMuBuY4+4nzexR4EbAgEnALHePm9mYxEYder/Y2UhnLM7KhRq3iEjyG+ip/yEg38y6gTBQB3wVuMnd4wDufigxEYNTGYkyuSTMovKioKOIiPSr3zmCu0eB+4EaoB5ocfcNwDTg98ysysyeNrPz3+/xZnZX3zZVTU1Ng5k9oRpaOnhp32FWLijTKkQikhL6LXQzKwZWAFOBCUCBmd0MDAM63L0C+D7wwPs93t1Xu3uFu1eUlqbOG4tPbovijsYtIpIyBvJO3zJgv7s3uXs3sA64FKgFHu/bZj0wLzERg7E+UseCSUVMHV0QdBQRkQEZSKHXAEvMLGy9s4elQDVQCVzVt80VwBuJiTj0dje0Ul3fyirtnYtICun3TVF332Rma4EtQAyIAKuBfODHZnYPcAK4M5FBh1JlpI7sLGP5vPFBRxERGbABHeXi7vcB973n5k7gukFPFLB43Hlia5QrZpRSMnxY0HFERAZMZ8u8x8b9zdS3dOjNUBFJOSr096iMRBk+LMTVs7UqkYikFhX6KTq6e3h6ewPXzh1Hfm520HFERD4QFfopfll9iOOdMR3dIiIpSYV+ivWRKGNHDGPJeSVBRxER+cBU6H2OtHXxwuuHWLGgjOwsneovIqlHhd7n59vricWdlVrIQkRSlAq9z/ottcwcW8js8YVBRxEROScqdOBAcxtbao6xapGurCgiqUuFTu+p/mbwqfkTgo4iInLOMr7Q3Z3KrVGWTC1hQlF+0HFERM5Zxhf6ttoW9h9u07HnIpLyMr7QKyNRckNZXHvhuKCjiIh8KBld6N09cX66rY6rZ49lRF5O0HFERD6UjC70F/c00dzWpSsrikhayOhCXx+pozicwxUzUmetUxGRM8nYQj/e0c2GnQ0snzeB3FDGvgwikkYytsl+sbORzlhc4xYRSRsZW+iVkSjlo8IsKi8KOoqIyKDIyEJvbO3gpX2HWblQp/qLSPrIyEJ/cmsd7rBygU71F5H0kZGFvi4SZf6kIs4rHR50FBGRQZNxhb67oZXq+lZWae9cRNLMgArdzO4xs51mtsPM1phZnpn9yMz2m9nWvj8LEh12MFRG6sjOMj6pKyuKSJoJ9beBmZUBdwNz3P2kmT0K3Nh3973uvjaRAQdTPO48sTXKFTNKKRk+LOg4IiKDaqAjlxCQb2YhIAzUJS5S4mzaf4T6lg4dey4iaanfQnf3KHA/UAPUAy3uvqHv7q+Z2Wtm9k0zS/pd3spIlILcbK6ePTboKCIig67fQjezYmAFMBWYABSY2c3Al4FZwMXAKOAvzvD4u8ysysyqmpqaBi34B9XR3cNT2+u5du548nOzA8shIpIoAxm5LAP2u3uTu3cD64BL3b3ee3UCDwKL3+/B7r7a3SvcvaK0NLiLYP2y+hDHO2NayEJE0tZACr0GWGJmYes9rXIpUG1m4wH6blsJ7EhczA9vfSTKmMJhXDKtJOgoIiIJ0e9RLu6+yczWAluAGBABVgNPm1kpYMBW4HOJDPphHGnr4oXXD3H7R6eQnaVT/UUkPfVb6ADufh9w33tuvmrw4yTGz7fXE4s7qxZODDqKiEjCZMSZopWRKDPHFjJ7fGHQUUREEibtC72muZ3NB47qyooikvbSvtArt0YBWKFrt4hImkvrQnd31keiLDlvFBOK8oOOIyKSUGld6NtqW9h/uE3HnotIRkjrQq+MRMkNZXHt3PFBRxERSbi0LfTunjg/3VbH1bPHMjI/J+g4IiIJl7aF/us9h2lu69KVFUUkY6Rtoa+PRCkK53DFjOCuHyMiMpTSstBPdMbYsKuB5fPGkxtKy/9FEZHfkJZt98yOBjq64zq6RUQySloWemUkyqRR+SwqLw46iojIkEm7Qm9s7eClfYdZtUCn+otIZkm7Qn9yax3u6OgWEck4aVfo6yNR5k8q4rzS4UFHEREZUmlV6K83HGdXfSurdCEuEclAaVXo6yNRsrOM5fNV6CKSedKm0ONx54mtUS4/fzSjhw8LOo6IyJBLm0LftP8I9S0dejNURDJW2hR6ZSRKQW42H58zLugoIiKBSItC7+ju4ant9Vwzdxz5udlBxxERCURaFPrzuw9xvDPG9QsnBh1FRCQwaVHo6yNRxhQO45JpJUFHEREJTMoX+tG2Ll54/RArFkwgO0un+otI5hpQoZvZPWa208x2mNkaM8s75b5vmdmJxEU8u59tr6e7x3V0i4hkvH4L3czKgLuBCnefC2QDN/bdVwEUJTRhPyojUWaMHc6c8SOCjCEiEriBjlxCQL6ZhYAwUGdm2cC/AF9KVLj+1DS3s/nAUVYu1JUVRUT6LXR3jwL3AzVAPdDi7huAPwGedPf6sz3ezO4ysyozq2pqahqMzO+o3BoFYMUCjVtERAYycikGVgBTgQlAgZndCtwAfKu/x7v7anevcPeK0tLBW9/T3amMRFly3ijKivIH7b8rIpKqBjJyWQbsd/cmd+8G1gF/B0wH9prZW0DYzPYmLuZveq22hTcPt2mZORGRPgMp9BpgiZmFrXdQvRT4hruPc/cp7j4FaHf36YkM+l7rI1FyQ1lcO3f8UD6tiEjSGsgMfROwFtgCbO97zOoE5zqr7p44P91Wx7LZYxiZnxNkFBGRpBEayEbufh9w31nuH9LlgX695zDNbV2s1JuhIiLvSMkzRddHohSFc/itmWOCjiIikjRSrtBPdMbYsKuB6y4cT24o5eKLiCRMyjXiL3Y00NEd5/pFGreIiJwq5Qq9cmuUSaPyWVReHHQUEZGkklKF3tjawUt7D7NqgU71FxF5r5Qq9Ce31hF3WKGTiUREfkNKFfr6SJT5E0cyrXRIj5IUEUkJKVPorzccZ1d9q657LiJyBilT6JVbo2RnGcvnTQg6iohIUkqJQo/HnSciUS47fzSlhcOCjiMikpRSotA37T9CXUuHrqwoInIWKVHolZEoBbnZfHzOuKCjiIgkrZQo9CmjC7j10ink52YHHUVEJGkN6GqLQfuj35oWdAQRkaSXEnvoIiLSPxW6iEiaUKGLiKQJFbqISJpQoYuIpAkVuohImlChi4ikCRW6iEiaMHcfuiczawIOnOPDRwOHBzFOqtPr8S69FqfT63G6dHg9Jrt7aX8bDWmhfxhmVuXuFUHnSBZ6Pd6l1+J0ej1Ol0mvh0YuIiJpQoUuIpImUqnQVwcdIMno9XiXXovT6fU4Xca8HikzQxcRkbNLpT10ERE5i5QodDO71sxeN7O9ZvaXQecJiplNMrNfmVm1me00sz8NOlMyMLNsM4uY2c+CzhI0Mysys7Vmtrvv6+SSoDMFxczu6fs+2WFma8wsL+hMiZb0hW5m2cB3gN8G5gCfNrM5waYKTAz4c3efDSwBPp/Br8Wp/hSoDjpEkvi/wDPuPguYT4a+LmZWBtwNVLj7XCAbuDHYVImX9IUOLAb2uvub7t4F/ARYEXCmQLh7vbtv6fv4OL3frBm9craZTQSuA34QdJagmdkI4HLghwDu3uXux4JNFagQkG9mISAM1AWcJ+FSodDLgIOnfF5LhpcYgJlNARYCm4JNErh/Bb4ExIMOkgTOA5qAB/tGUD8ws4KgQwXB3aPA/UANUA+0uPuGYFMlXioUur3PbRl9aI6ZDQceB/7M3VuDzhMUM1sOHHL3zUFnSRIhYBHwXXdfCLQBGfmek5kV0/ub/FRgAlBgZjcHmyrxUqHQa4FJp3w+kQz41elMzCyH3jL/sbuvCzpPwD4KfMrM3qJ3FHeVmT0SbKRA1QK17v72b21r6S34TLQM2O/uTe7eDawDLg04U8KlQqG/CpxvZlPNLJfeNzaeDDhTIMzM6J2PVrv7N4LOEzR3/7K7T3T3KfR+XTzv7mm/F3Ym7t4AHDSzmX03LQV2BRgpSDXAEjML933fLCUD3iAOBR2gP+4eM7M/AX5B7zvVD7j7zoBjBeWjwC3AdjPb2nfbX7n7UwFmkuTyBeDHfTs/bwK3B5wnEO6+yczWAlvoPTosQgacMaozRUVE0kQqjFxERGQAVOgiImlChS4ikiZU6CIiaUKFLiKSJlToIiJpQoUuIpImVOgiImni/wN1aCoC+bh6UwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracy_vals_wnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random shuffling of data is also important\n",
    "# random_idxs = torch.randperm(train_n) \n",
    "# start_i = i*bs\n",
    "# end_i = start_i+bs\n",
    "# xb = x_train[random_idxs[start_i:end_i]].reshape(bs, 1, 28, 28)\n",
    "# yb = y_train[random_idxs[start_i:end_i]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning most important ``hyper-parameter`` for training neural networks - learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n",
    "import math\n",
    "def find_lr(net, loss_func, init_value = 1e-8, final_value=10., beta = 0.98, bs = 32):\n",
    "    num = (train_n-1)//bs + 1 # num of batches \n",
    "    mult = (final_value/init_value) ** (1/num)\n",
    "    lr = init_value\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "    avg_loss = 0.\n",
    "    best_loss = 0.\n",
    "    batch_num = 0.\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    for i in range((train_n-1)//bs + 1):\n",
    "        batch_num += 1\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xb = x_train[start_i:end_i].reshape(bs, 1, 28, 28)\n",
    "        yb = y_train[start_i:end_i]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net.forward(xb)\n",
    "        loss = loss_func(outputs, yb)\n",
    "        #Compute the smoothed loss\n",
    "        print(\"loss: \", loss.item())\n",
    "        avg_loss = beta * avg_loss + (1-beta) *loss.item()\n",
    "        smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "        #Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
    "            return log_lrs, losses\n",
    "        #Record the best loss\n",
    "        if smoothed_loss < best_loss or batch_num==1:\n",
    "            best_loss = smoothed_loss\n",
    "        #Store the values\n",
    "        losses.append(smoothed_loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "        #Do the SGD step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #Update the lr for the next step\n",
    "        lr *= mult\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "    return log_lrs, losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.312309503555298\n",
      "loss:  2.2941360473632812\n",
      "loss:  2.310781955718994\n",
      "loss:  2.288942337036133\n",
      "loss:  2.2957277297973633\n",
      "loss:  2.313990831375122\n",
      "loss:  2.2955784797668457\n",
      "loss:  2.3203577995300293\n",
      "loss:  2.2942047119140625\n",
      "loss:  2.303245782852173\n",
      "loss:  2.323859930038452\n",
      "loss:  2.2928340435028076\n",
      "loss:  2.288990020751953\n",
      "loss:  2.299994707107544\n",
      "loss:  2.297529697418213\n",
      "loss:  2.2869608402252197\n",
      "loss:  2.3063817024230957\n",
      "loss:  2.3111870288848877\n",
      "loss:  2.300764322280884\n",
      "loss:  2.307935953140259\n",
      "loss:  2.316350221633911\n",
      "loss:  2.303212881088257\n",
      "loss:  2.316728353500366\n",
      "loss:  2.3041224479675293\n",
      "loss:  2.2896509170532227\n",
      "loss:  2.325892448425293\n",
      "loss:  2.30553936958313\n",
      "loss:  2.3159382343292236\n",
      "loss:  2.2936911582946777\n",
      "loss:  2.314988374710083\n",
      "loss:  2.2934417724609375\n",
      "loss:  2.3162968158721924\n",
      "loss:  2.3004918098449707\n",
      "loss:  2.308074712753296\n",
      "loss:  2.2993757724761963\n",
      "loss:  2.2933125495910645\n",
      "loss:  2.317561149597168\n",
      "loss:  2.2878048419952393\n",
      "loss:  2.305246591567993\n",
      "loss:  2.3116190433502197\n",
      "loss:  2.322434902191162\n",
      "loss:  2.314044952392578\n",
      "loss:  2.3179104328155518\n",
      "loss:  2.306694746017456\n",
      "loss:  2.2835848331451416\n",
      "loss:  2.286285638809204\n",
      "loss:  2.2949392795562744\n",
      "loss:  2.304509162902832\n",
      "loss:  2.3100767135620117\n",
      "loss:  2.3016090393066406\n",
      "loss:  2.3113741874694824\n",
      "loss:  2.2975144386291504\n",
      "loss:  2.295154333114624\n",
      "loss:  2.2985103130340576\n",
      "loss:  2.309380054473877\n",
      "loss:  2.309220790863037\n",
      "loss:  2.3129830360412598\n",
      "loss:  2.288771152496338\n",
      "loss:  2.308598279953003\n",
      "loss:  2.3054616451263428\n",
      "loss:  2.2900376319885254\n",
      "loss:  2.2972776889801025\n",
      "loss:  2.3180696964263916\n",
      "loss:  2.292560338973999\n",
      "loss:  2.299363851547241\n",
      "loss:  2.303643226623535\n",
      "loss:  2.2978217601776123\n",
      "loss:  2.3045854568481445\n",
      "loss:  2.304154872894287\n",
      "loss:  2.312957763671875\n",
      "loss:  2.315758466720581\n",
      "loss:  2.305619239807129\n",
      "loss:  2.2978460788726807\n",
      "loss:  2.289989709854126\n",
      "loss:  2.295741558074951\n",
      "loss:  2.306312322616577\n",
      "loss:  2.3114187717437744\n",
      "loss:  2.297084331512451\n",
      "loss:  2.3052687644958496\n",
      "loss:  2.3073525428771973\n",
      "loss:  2.3061230182647705\n",
      "loss:  2.2969510555267334\n",
      "loss:  2.3195767402648926\n",
      "loss:  2.2963876724243164\n",
      "loss:  2.298312187194824\n",
      "loss:  2.3247005939483643\n",
      "loss:  2.27763032913208\n",
      "loss:  2.296334981918335\n",
      "loss:  2.3205153942108154\n",
      "loss:  2.3003039360046387\n",
      "loss:  2.295262336730957\n",
      "loss:  2.3102641105651855\n",
      "loss:  2.316549301147461\n",
      "loss:  2.32012939453125\n",
      "loss:  2.299677848815918\n",
      "loss:  2.311366558074951\n",
      "loss:  2.292240858078003\n",
      "loss:  2.2894608974456787\n",
      "loss:  2.3240349292755127\n",
      "loss:  2.284346103668213\n",
      "loss:  2.296173334121704\n",
      "loss:  2.3061623573303223\n",
      "loss:  2.3004226684570312\n",
      "loss:  2.2873146533966064\n",
      "loss:  2.3249218463897705\n",
      "loss:  2.3086071014404297\n",
      "loss:  2.2912702560424805\n",
      "loss:  2.308445930480957\n",
      "loss:  2.3076300621032715\n",
      "loss:  2.2889037132263184\n",
      "loss:  2.3091983795166016\n",
      "loss:  2.2992606163024902\n",
      "loss:  2.3026926517486572\n",
      "loss:  2.302776575088501\n",
      "loss:  2.318932056427002\n",
      "loss:  2.315965414047241\n",
      "loss:  2.2889938354492188\n",
      "loss:  2.3105485439300537\n",
      "loss:  2.295262575149536\n",
      "loss:  2.275456190109253\n",
      "loss:  2.2817158699035645\n",
      "loss:  2.302525758743286\n",
      "loss:  2.3012099266052246\n",
      "loss:  2.288823127746582\n",
      "loss:  2.317800521850586\n",
      "loss:  2.3095057010650635\n",
      "loss:  2.3037235736846924\n",
      "loss:  2.2884860038757324\n",
      "loss:  2.300938844680786\n",
      "loss:  2.299243688583374\n",
      "loss:  2.2969017028808594\n",
      "loss:  2.309387683868408\n",
      "loss:  2.3155124187469482\n",
      "loss:  2.3056576251983643\n",
      "loss:  2.326977491378784\n",
      "loss:  2.3110673427581787\n",
      "loss:  2.296762228012085\n",
      "loss:  2.310616970062256\n",
      "loss:  2.3068668842315674\n",
      "loss:  2.2977147102355957\n",
      "loss:  2.293874979019165\n",
      "loss:  2.2967495918273926\n",
      "loss:  2.302271604537964\n",
      "loss:  2.3043227195739746\n",
      "loss:  2.293438196182251\n",
      "loss:  2.306845188140869\n",
      "loss:  2.2906432151794434\n",
      "loss:  2.310488700866699\n",
      "loss:  2.298020124435425\n",
      "loss:  2.3036303520202637\n",
      "loss:  2.308321952819824\n",
      "loss:  2.2950918674468994\n",
      "loss:  2.2881369590759277\n",
      "loss:  2.285749673843384\n",
      "loss:  2.312103271484375\n",
      "loss:  2.3031585216522217\n",
      "loss:  2.3102619647979736\n",
      "loss:  2.291760206222534\n",
      "loss:  2.2944328784942627\n",
      "loss:  2.305034875869751\n",
      "loss:  2.3097355365753174\n",
      "loss:  2.3009607791900635\n",
      "loss:  2.3139615058898926\n",
      "loss:  2.307206869125366\n",
      "loss:  2.302607774734497\n",
      "loss:  2.3011722564697266\n",
      "loss:  2.3065507411956787\n",
      "loss:  2.300004720687866\n",
      "loss:  2.3070218563079834\n",
      "loss:  2.302837371826172\n",
      "loss:  2.325035572052002\n",
      "loss:  2.279674530029297\n",
      "loss:  2.300433397293091\n",
      "loss:  2.318185567855835\n",
      "loss:  2.2970173358917236\n",
      "loss:  2.3167710304260254\n",
      "loss:  2.3150575160980225\n",
      "loss:  2.2937886714935303\n",
      "loss:  2.288160562515259\n",
      "loss:  2.2946908473968506\n",
      "loss:  2.321288585662842\n",
      "loss:  2.3033013343811035\n",
      "loss:  2.2936151027679443\n",
      "loss:  2.3020801544189453\n",
      "loss:  2.2960381507873535\n",
      "loss:  2.296304702758789\n",
      "loss:  2.285311698913574\n",
      "loss:  2.3150298595428467\n",
      "loss:  2.310640573501587\n",
      "loss:  2.3017687797546387\n",
      "loss:  2.288705587387085\n",
      "loss:  2.2948765754699707\n",
      "loss:  2.3079833984375\n",
      "loss:  2.2785181999206543\n",
      "loss:  2.3037400245666504\n",
      "loss:  2.3135995864868164\n",
      "loss:  2.2962779998779297\n",
      "loss:  2.3077142238616943\n",
      "loss:  2.2785868644714355\n",
      "loss:  2.2980053424835205\n",
      "loss:  2.287106513977051\n",
      "loss:  2.3069908618927\n",
      "loss:  2.2831151485443115\n",
      "loss:  2.310415744781494\n",
      "loss:  2.3146674633026123\n",
      "loss:  2.3053812980651855\n",
      "loss:  2.294574499130249\n",
      "loss:  2.289283514022827\n",
      "loss:  2.3153839111328125\n",
      "loss:  2.299877643585205\n",
      "loss:  2.3299436569213867\n",
      "loss:  2.319129705429077\n",
      "loss:  2.3100788593292236\n",
      "loss:  2.30761981010437\n",
      "loss:  2.2952916622161865\n",
      "loss:  2.2968311309814453\n",
      "loss:  2.3068246841430664\n",
      "loss:  2.3008553981781006\n",
      "loss:  2.3059279918670654\n",
      "loss:  2.301490306854248\n",
      "loss:  2.2861557006835938\n",
      "loss:  2.330260992050171\n",
      "loss:  2.276212692260742\n",
      "loss:  2.3014631271362305\n",
      "loss:  2.298630475997925\n",
      "loss:  2.2837681770324707\n",
      "loss:  2.2960400581359863\n",
      "loss:  2.3026866912841797\n",
      "loss:  2.2805368900299072\n",
      "loss:  2.3085856437683105\n",
      "loss:  2.3275322914123535\n",
      "loss:  2.311480760574341\n",
      "loss:  2.287569761276245\n",
      "loss:  2.276648998260498\n",
      "loss:  2.294283628463745\n",
      "loss:  2.311830759048462\n",
      "loss:  2.2962534427642822\n",
      "loss:  2.316345691680908\n",
      "loss:  2.3160033226013184\n",
      "loss:  2.3136775493621826\n",
      "loss:  2.3046059608459473\n",
      "loss:  2.321502208709717\n",
      "loss:  2.2900145053863525\n",
      "loss:  2.3219826221466064\n",
      "loss:  2.29946231842041\n",
      "loss:  2.3013553619384766\n",
      "loss:  2.3140997886657715\n",
      "loss:  2.3075287342071533\n",
      "loss:  2.295041799545288\n",
      "loss:  2.2807393074035645\n",
      "loss:  2.3065438270568848\n",
      "loss:  2.313211679458618\n",
      "loss:  2.3036460876464844\n",
      "loss:  2.3229477405548096\n",
      "loss:  2.2930054664611816\n",
      "loss:  2.3075637817382812\n",
      "loss:  2.3027381896972656\n",
      "loss:  2.300199270248413\n",
      "loss:  2.3070545196533203\n",
      "loss:  2.313567638397217\n",
      "loss:  2.314810276031494\n",
      "loss:  2.302323818206787\n",
      "loss:  2.318963050842285\n",
      "loss:  2.2795019149780273\n",
      "loss:  2.3174123764038086\n",
      "loss:  2.289320945739746\n",
      "loss:  2.297389507293701\n",
      "loss:  2.297133445739746\n",
      "loss:  2.312675714492798\n",
      "loss:  2.30087947845459\n",
      "loss:  2.3047609329223633\n",
      "loss:  2.2970945835113525\n",
      "loss:  2.311668634414673\n",
      "loss:  2.309685707092285\n",
      "loss:  2.272357940673828\n",
      "loss:  2.2838211059570312\n",
      "loss:  2.2932355403900146\n",
      "loss:  2.318035364151001\n",
      "loss:  2.2800188064575195\n",
      "loss:  2.3090319633483887\n",
      "loss:  2.293119430541992\n",
      "loss:  2.3082852363586426\n",
      "loss:  2.2934019565582275\n",
      "loss:  2.3175699710845947\n",
      "loss:  2.294119358062744\n",
      "loss:  2.3158509731292725\n",
      "loss:  2.2845375537872314\n",
      "loss:  2.3100905418395996\n",
      "loss:  2.2915637493133545\n",
      "loss:  2.3072540760040283\n",
      "loss:  2.316873788833618\n",
      "loss:  2.3267621994018555\n",
      "loss:  2.2821919918060303\n",
      "loss:  2.2882895469665527\n",
      "loss:  2.285285472869873\n",
      "loss:  2.2828128337860107\n",
      "loss:  2.302243232727051\n",
      "loss:  2.300478935241699\n",
      "loss:  2.3230621814727783\n",
      "loss:  2.300213098526001\n",
      "loss:  2.315091133117676\n",
      "loss:  2.3057103157043457\n",
      "loss:  2.314345121383667\n",
      "loss:  2.3070967197418213\n",
      "loss:  2.3142642974853516\n",
      "loss:  2.3046083450317383\n",
      "loss:  2.28717041015625\n",
      "loss:  2.288116693496704\n",
      "loss:  2.3138959407806396\n",
      "loss:  2.304114580154419\n",
      "loss:  2.3077800273895264\n",
      "loss:  2.2999351024627686\n",
      "loss:  2.317858934402466\n",
      "loss:  2.3056256771087646\n",
      "loss:  2.3013858795166016\n",
      "loss:  2.312631130218506\n",
      "loss:  2.289726734161377\n",
      "loss:  2.3244919776916504\n",
      "loss:  2.3121678829193115\n",
      "loss:  2.2918872833251953\n",
      "loss:  2.30586838722229\n",
      "loss:  2.321237564086914\n",
      "loss:  2.2970781326293945\n",
      "loss:  2.2971444129943848\n",
      "loss:  2.3089370727539062\n",
      "loss:  2.28059983253479\n",
      "loss:  2.317878246307373\n",
      "loss:  2.306852340698242\n",
      "loss:  2.305739402770996\n",
      "loss:  2.303046703338623\n",
      "loss:  2.2922630310058594\n",
      "loss:  2.293090343475342\n",
      "loss:  2.2934744358062744\n",
      "loss:  2.3004226684570312\n",
      "loss:  2.3070425987243652\n",
      "loss:  2.3101654052734375\n",
      "loss:  2.304680824279785\n",
      "loss:  2.294095516204834\n",
      "loss:  2.2842793464660645\n",
      "loss:  2.310136318206787\n",
      "loss:  2.3118674755096436\n",
      "loss:  2.3108203411102295\n",
      "loss:  2.294240951538086\n",
      "loss:  2.2888007164001465\n",
      "loss:  2.31142258644104\n",
      "loss:  2.31113338470459\n",
      "loss:  2.2955358028411865\n",
      "loss:  2.3130033016204834\n",
      "loss:  2.3232343196868896\n",
      "loss:  2.292339324951172\n",
      "loss:  2.307654619216919\n",
      "loss:  2.284470796585083\n",
      "loss:  2.2877557277679443\n",
      "loss:  2.3102691173553467\n",
      "loss:  2.3077704906463623\n",
      "loss:  2.3078296184539795\n",
      "loss:  2.301375150680542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.3195180892944336\n",
      "loss:  2.3160343170166016\n",
      "loss:  2.302102565765381\n",
      "loss:  2.3073668479919434\n",
      "loss:  2.304689407348633\n",
      "loss:  2.307504892349243\n",
      "loss:  2.2920403480529785\n",
      "loss:  2.3052420616149902\n",
      "loss:  2.306729793548584\n",
      "loss:  2.2865633964538574\n",
      "loss:  2.313988208770752\n",
      "loss:  2.3009042739868164\n",
      "loss:  2.2927422523498535\n",
      "loss:  2.3162660598754883\n",
      "loss:  2.3192522525787354\n",
      "loss:  2.305509567260742\n",
      "loss:  2.3054800033569336\n",
      "loss:  2.3234453201293945\n",
      "loss:  2.3165879249572754\n",
      "loss:  2.2841761112213135\n",
      "loss:  2.3079957962036133\n",
      "loss:  2.2870631217956543\n",
      "loss:  2.2809672355651855\n",
      "loss:  2.2898776531219482\n",
      "loss:  2.316314935684204\n",
      "loss:  2.307500123977661\n",
      "loss:  2.2943530082702637\n",
      "loss:  2.313481569290161\n",
      "loss:  2.311671018600464\n",
      "loss:  2.295557737350464\n",
      "loss:  2.2940587997436523\n",
      "loss:  2.297394275665283\n",
      "loss:  2.328916311264038\n",
      "loss:  2.3013460636138916\n",
      "loss:  2.2949435710906982\n",
      "loss:  2.2933456897735596\n",
      "loss:  2.3129465579986572\n",
      "loss:  2.3029751777648926\n",
      "loss:  2.302544593811035\n",
      "loss:  2.3033902645111084\n",
      "loss:  2.2918829917907715\n",
      "loss:  2.3163673877716064\n",
      "loss:  2.3070850372314453\n",
      "loss:  2.3083372116088867\n",
      "loss:  2.3160557746887207\n",
      "loss:  2.3030571937561035\n",
      "loss:  2.315384864807129\n",
      "loss:  2.286882162094116\n",
      "loss:  2.286306619644165\n",
      "loss:  2.305814027786255\n",
      "loss:  2.289266347885132\n",
      "loss:  2.2890353202819824\n",
      "loss:  2.289863348007202\n",
      "loss:  2.3002986907958984\n",
      "loss:  2.2960243225097656\n",
      "loss:  2.2970407009124756\n",
      "loss:  2.3160808086395264\n",
      "loss:  2.319092273712158\n",
      "loss:  2.304900884628296\n",
      "loss:  2.323317289352417\n",
      "loss:  2.2934041023254395\n",
      "loss:  2.286470413208008\n",
      "loss:  2.2809948921203613\n",
      "loss:  2.310372829437256\n",
      "loss:  2.3035147190093994\n",
      "loss:  2.308025598526001\n",
      "loss:  2.2844371795654297\n",
      "loss:  2.302403211593628\n",
      "loss:  2.322422742843628\n",
      "loss:  2.2920162677764893\n",
      "loss:  2.3005406856536865\n",
      "loss:  2.296334981918335\n",
      "loss:  2.312375068664551\n",
      "loss:  2.2978343963623047\n",
      "loss:  2.316286563873291\n",
      "loss:  2.3120803833007812\n",
      "loss:  2.322641134262085\n",
      "loss:  2.2955760955810547\n",
      "loss:  2.30554461479187\n",
      "loss:  2.2908785343170166\n",
      "loss:  2.316699981689453\n",
      "loss:  2.3211801052093506\n",
      "loss:  2.3046460151672363\n",
      "loss:  2.3154304027557373\n",
      "loss:  2.2897815704345703\n",
      "loss:  2.3040735721588135\n",
      "loss:  2.317422866821289\n",
      "loss:  2.2932779788970947\n",
      "loss:  2.310847520828247\n",
      "loss:  2.308832883834839\n",
      "loss:  2.310652256011963\n",
      "loss:  2.2960293292999268\n",
      "loss:  2.308274745941162\n",
      "loss:  2.336899995803833\n",
      "loss:  2.321305274963379\n",
      "loss:  2.3030221462249756\n",
      "loss:  2.29477596282959\n",
      "loss:  2.3178515434265137\n",
      "loss:  2.3088083267211914\n",
      "loss:  2.304442882537842\n",
      "loss:  2.308896780014038\n",
      "loss:  2.2912726402282715\n",
      "loss:  2.295065402984619\n",
      "loss:  2.3232221603393555\n",
      "loss:  2.3057286739349365\n",
      "loss:  2.3174149990081787\n",
      "loss:  2.303574800491333\n",
      "loss:  2.311901330947876\n",
      "loss:  2.304095983505249\n",
      "loss:  2.286811113357544\n",
      "loss:  2.2977426052093506\n",
      "loss:  2.324924945831299\n",
      "loss:  2.307976722717285\n",
      "loss:  2.317106246948242\n",
      "loss:  2.305344820022583\n",
      "loss:  2.3084659576416016\n",
      "loss:  2.2965145111083984\n",
      "loss:  2.307518482208252\n",
      "loss:  2.2853457927703857\n",
      "loss:  2.311079740524292\n",
      "loss:  2.305739164352417\n",
      "loss:  2.297457456588745\n",
      "loss:  2.298518419265747\n",
      "loss:  2.323228120803833\n",
      "loss:  2.3129477500915527\n",
      "loss:  2.3102192878723145\n",
      "loss:  2.313052177429199\n",
      "loss:  2.31117582321167\n",
      "loss:  2.298999071121216\n",
      "loss:  2.3043620586395264\n",
      "loss:  2.308211088180542\n",
      "loss:  2.3198630809783936\n",
      "loss:  2.291623115539551\n",
      "loss:  2.2894248962402344\n",
      "loss:  2.298292875289917\n",
      "loss:  2.304150342941284\n",
      "loss:  2.2973334789276123\n",
      "loss:  2.3103880882263184\n",
      "loss:  2.289316415786743\n",
      "loss:  2.2939915657043457\n",
      "loss:  2.3106682300567627\n",
      "loss:  2.2797133922576904\n",
      "loss:  2.304677963256836\n",
      "loss:  2.3070664405822754\n",
      "loss:  2.3070592880249023\n",
      "loss:  2.2962276935577393\n",
      "loss:  2.291461229324341\n",
      "loss:  2.300403594970703\n",
      "loss:  2.3246524333953857\n",
      "loss:  2.303844690322876\n",
      "loss:  2.3054025173187256\n",
      "loss:  2.300229549407959\n",
      "loss:  2.3286492824554443\n",
      "loss:  2.300900936126709\n",
      "loss:  2.295545816421509\n",
      "loss:  2.2884535789489746\n",
      "loss:  2.309352397918701\n",
      "loss:  2.30401349067688\n",
      "loss:  2.3081302642822266\n",
      "loss:  2.3054068088531494\n",
      "loss:  2.3054146766662598\n",
      "loss:  2.3012166023254395\n",
      "loss:  2.3192391395568848\n",
      "loss:  2.307185649871826\n",
      "loss:  2.3064980506896973\n",
      "loss:  2.2808148860931396\n",
      "loss:  2.2967357635498047\n",
      "loss:  2.3032119274139404\n",
      "loss:  2.3114736080169678\n",
      "loss:  2.286371946334839\n",
      "loss:  2.304788827896118\n",
      "loss:  2.290905475616455\n",
      "loss:  2.3289148807525635\n",
      "loss:  2.2807986736297607\n",
      "loss:  2.308586359024048\n",
      "loss:  2.292212963104248\n",
      "loss:  2.2925891876220703\n",
      "loss:  2.286707639694214\n",
      "loss:  2.2953174114227295\n",
      "loss:  2.29860520362854\n",
      "loss:  2.300701141357422\n",
      "loss:  2.3175573348999023\n",
      "loss:  2.3131155967712402\n",
      "loss:  2.3046700954437256\n",
      "loss:  2.3106634616851807\n",
      "loss:  2.320711374282837\n",
      "loss:  2.3040060997009277\n",
      "loss:  2.290015697479248\n",
      "loss:  2.3113973140716553\n",
      "loss:  2.3341662883758545\n",
      "loss:  2.2949819564819336\n",
      "loss:  2.2970781326293945\n",
      "loss:  2.3187384605407715\n",
      "loss:  2.302659511566162\n",
      "loss:  2.301501989364624\n",
      "loss:  2.315807580947876\n",
      "loss:  2.299783706665039\n",
      "loss:  2.306353807449341\n",
      "loss:  2.297806739807129\n",
      "loss:  2.3023970127105713\n",
      "loss:  2.3066346645355225\n",
      "loss:  2.2888190746307373\n",
      "loss:  2.3321588039398193\n",
      "loss:  2.2994115352630615\n",
      "loss:  2.2992336750030518\n",
      "loss:  2.3188652992248535\n",
      "loss:  2.2937912940979004\n",
      "loss:  2.305299997329712\n",
      "loss:  2.311023473739624\n",
      "loss:  2.3136513233184814\n",
      "loss:  2.304075241088867\n",
      "loss:  2.307652235031128\n",
      "loss:  2.3089048862457275\n",
      "loss:  2.3077282905578613\n",
      "loss:  2.286714792251587\n",
      "loss:  2.3173916339874268\n",
      "loss:  2.3006162643432617\n",
      "loss:  2.3061795234680176\n",
      "loss:  2.301142692565918\n",
      "loss:  2.301164150238037\n",
      "loss:  2.2942869663238525\n",
      "loss:  2.3050010204315186\n",
      "loss:  2.3057494163513184\n",
      "loss:  2.276979923248291\n",
      "loss:  2.2868804931640625\n",
      "loss:  2.302351951599121\n",
      "loss:  2.303382158279419\n",
      "loss:  2.285214900970459\n",
      "loss:  2.2968039512634277\n",
      "loss:  2.3142001628875732\n",
      "loss:  2.2920851707458496\n",
      "loss:  2.299389362335205\n",
      "loss:  2.3177077770233154\n",
      "loss:  2.2869269847869873\n",
      "loss:  2.2785792350769043\n",
      "loss:  2.2967660427093506\n",
      "loss:  2.30332088470459\n",
      "loss:  2.3045060634613037\n",
      "loss:  2.3187179565429688\n",
      "loss:  2.30924129486084\n",
      "loss:  2.3021609783172607\n",
      "loss:  2.288848400115967\n",
      "loss:  2.312573194503784\n",
      "loss:  2.3130831718444824\n",
      "loss:  2.3221375942230225\n",
      "loss:  2.28963565826416\n",
      "loss:  2.3070969581604004\n",
      "loss:  2.2997548580169678\n",
      "loss:  2.2972652912139893\n",
      "loss:  2.3081276416778564\n",
      "loss:  2.3013784885406494\n",
      "loss:  2.3257102966308594\n",
      "loss:  2.293879508972168\n",
      "loss:  2.304842472076416\n",
      "loss:  2.303556442260742\n",
      "loss:  2.321082353591919\n",
      "loss:  2.2884325981140137\n",
      "loss:  2.2920093536376953\n",
      "loss:  2.2982232570648193\n",
      "loss:  2.3001654148101807\n",
      "loss:  2.3155765533447266\n",
      "loss:  2.3098747730255127\n",
      "loss:  2.3011059761047363\n",
      "loss:  2.309440851211548\n",
      "loss:  2.3056349754333496\n",
      "loss:  2.311917304992676\n",
      "loss:  2.2966859340667725\n",
      "loss:  2.296621322631836\n",
      "loss:  2.308499574661255\n",
      "loss:  2.2896721363067627\n",
      "loss:  2.3037827014923096\n",
      "loss:  2.3087332248687744\n",
      "loss:  2.3130736351013184\n",
      "loss:  2.2961199283599854\n",
      "loss:  2.2896831035614014\n",
      "loss:  2.2926900386810303\n",
      "loss:  2.292634963989258\n",
      "loss:  2.2972097396850586\n",
      "loss:  2.3062260150909424\n",
      "loss:  2.3093440532684326\n",
      "loss:  2.3082430362701416\n",
      "loss:  2.294097423553467\n",
      "loss:  2.2982983589172363\n",
      "loss:  2.3049933910369873\n",
      "loss:  2.299280881881714\n",
      "loss:  2.3333191871643066\n",
      "loss:  2.2916982173919678\n",
      "loss:  2.2957589626312256\n",
      "loss:  2.3102643489837646\n",
      "loss:  2.319643974304199\n",
      "loss:  2.2857561111450195\n",
      "loss:  2.3009417057037354\n",
      "loss:  2.307720422744751\n",
      "loss:  2.2894480228424072\n",
      "loss:  2.3122055530548096\n",
      "loss:  2.296934127807617\n",
      "loss:  2.324718475341797\n",
      "loss:  2.2955105304718018\n",
      "loss:  2.3048317432403564\n",
      "loss:  2.3047597408294678\n",
      "loss:  2.315748453140259\n",
      "loss:  2.3160102367401123\n",
      "loss:  2.31197190284729\n",
      "loss:  2.293508529663086\n",
      "loss:  2.291593074798584\n",
      "loss:  2.3023083209991455\n",
      "loss:  2.304391860961914\n",
      "loss:  2.3096160888671875\n",
      "loss:  2.317384719848633\n",
      "loss:  2.301980972290039\n",
      "loss:  2.2972657680511475\n",
      "loss:  2.3170199394226074\n",
      "loss:  2.307033061981201\n",
      "loss:  2.2833776473999023\n",
      "loss:  2.2965190410614014\n",
      "loss:  2.306060552597046\n",
      "loss:  2.295274257659912\n",
      "loss:  2.328523874282837\n",
      "loss:  2.28682541847229\n",
      "loss:  2.2979328632354736\n",
      "loss:  2.295438051223755\n",
      "loss:  2.295783519744873\n",
      "loss:  2.312507390975952\n",
      "loss:  2.301396369934082\n",
      "loss:  2.3138465881347656\n",
      "loss:  2.303123712539673\n",
      "loss:  2.3032586574554443\n",
      "loss:  2.2975172996520996\n",
      "loss:  2.292726993560791\n",
      "loss:  2.3191518783569336\n",
      "loss:  2.299161911010742\n",
      "loss:  2.2975165843963623\n",
      "loss:  2.3162684440612793\n",
      "loss:  2.3226306438446045\n",
      "loss:  2.3287386894226074\n",
      "loss:  2.3287224769592285\n",
      "loss:  2.310840368270874\n",
      "loss:  2.311842203140259\n",
      "loss:  2.292545795440674\n",
      "loss:  2.321054697036743\n",
      "loss:  2.3080251216888428\n",
      "loss:  2.3109209537506104\n",
      "loss:  2.3007431030273438\n",
      "loss:  2.299616813659668\n",
      "loss:  2.3034417629241943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.2823023796081543\n",
      "loss:  2.2866621017456055\n",
      "loss:  2.3095293045043945\n",
      "loss:  2.321329355239868\n",
      "loss:  2.2896976470947266\n",
      "loss:  2.3177597522735596\n",
      "loss:  2.2851548194885254\n",
      "loss:  2.2906408309936523\n",
      "loss:  2.3027215003967285\n",
      "loss:  2.273465871810913\n",
      "loss:  2.3089778423309326\n",
      "loss:  2.3009181022644043\n",
      "loss:  2.297107458114624\n",
      "loss:  2.30472469329834\n",
      "loss:  2.2849903106689453\n",
      "loss:  2.3094911575317383\n",
      "loss:  2.2992961406707764\n",
      "loss:  2.2959282398223877\n",
      "loss:  2.3044605255126953\n",
      "loss:  2.31477427482605\n",
      "loss:  2.28926420211792\n",
      "loss:  2.297349691390991\n",
      "loss:  2.3147530555725098\n",
      "loss:  2.310495376586914\n",
      "loss:  2.329799175262451\n",
      "loss:  2.2916362285614014\n",
      "loss:  2.2986061573028564\n",
      "loss:  2.3054144382476807\n",
      "loss:  2.294731378555298\n",
      "loss:  2.310957431793213\n",
      "loss:  2.3158488273620605\n",
      "loss:  2.299262285232544\n",
      "loss:  2.2898619174957275\n",
      "loss:  2.2907021045684814\n",
      "loss:  2.288590908050537\n",
      "loss:  2.3050472736358643\n",
      "loss:  2.3100552558898926\n",
      "loss:  2.2938952445983887\n",
      "loss:  2.28475284576416\n",
      "loss:  2.3014392852783203\n",
      "loss:  2.2825570106506348\n",
      "loss:  2.312666177749634\n",
      "loss:  2.287980318069458\n",
      "loss:  2.2998218536376953\n",
      "loss:  2.299983024597168\n",
      "loss:  2.2874577045440674\n",
      "loss:  2.2931461334228516\n",
      "loss:  2.320157051086426\n",
      "loss:  2.3591465950012207\n",
      "loss:  2.3207600116729736\n",
      "loss:  2.3029298782348633\n",
      "loss:  2.309051036834717\n",
      "loss:  2.3166706562042236\n",
      "loss:  2.3035337924957275\n",
      "loss:  2.320929527282715\n",
      "loss:  2.3069138526916504\n",
      "loss:  2.303795337677002\n",
      "loss:  2.316539764404297\n",
      "loss:  2.305293321609497\n",
      "loss:  2.3011550903320312\n",
      "loss:  2.325742721557617\n",
      "loss:  2.306410789489746\n",
      "loss:  2.3145408630371094\n",
      "loss:  2.3132762908935547\n",
      "loss:  2.2982594966888428\n",
      "loss:  2.292834997177124\n",
      "loss:  2.2964932918548584\n",
      "loss:  2.296834707260132\n",
      "loss:  2.2812204360961914\n",
      "loss:  2.309549331665039\n",
      "loss:  2.2997758388519287\n",
      "loss:  2.3057830333709717\n",
      "loss:  2.3032450675964355\n",
      "loss:  2.314898729324341\n",
      "loss:  2.3229846954345703\n",
      "loss:  2.292285919189453\n",
      "loss:  2.287504196166992\n",
      "loss:  2.2848052978515625\n",
      "loss:  2.302687883377075\n",
      "loss:  2.2901344299316406\n",
      "loss:  2.318044662475586\n",
      "loss:  2.2968590259552\n",
      "loss:  2.3298962116241455\n",
      "loss:  2.3049733638763428\n",
      "loss:  2.3115880489349365\n",
      "loss:  2.2908425331115723\n",
      "loss:  2.2788519859313965\n",
      "loss:  2.313079357147217\n",
      "loss:  2.303642749786377\n",
      "loss:  2.2989087104797363\n",
      "loss:  2.312885284423828\n",
      "loss:  2.3039135932922363\n",
      "loss:  2.3095576763153076\n",
      "loss:  2.317100763320923\n",
      "loss:  2.2944912910461426\n",
      "loss:  2.3171770572662354\n",
      "loss:  2.3199095726013184\n",
      "loss:  2.3003318309783936\n",
      "loss:  2.312211513519287\n",
      "loss:  2.3020589351654053\n",
      "loss:  2.2881383895874023\n",
      "loss:  2.2882702350616455\n",
      "loss:  2.2887535095214844\n",
      "loss:  2.2907958030700684\n",
      "loss:  2.318230152130127\n",
      "loss:  2.3057913780212402\n",
      "loss:  2.300889492034912\n",
      "loss:  2.3121604919433594\n",
      "loss:  2.306978702545166\n",
      "loss:  2.307654619216919\n",
      "loss:  2.3127267360687256\n",
      "loss:  2.327031373977661\n",
      "loss:  2.3082454204559326\n",
      "loss:  2.30755352973938\n",
      "loss:  2.31150221824646\n",
      "loss:  2.3129043579101562\n",
      "loss:  2.28066349029541\n",
      "loss:  2.3223459720611572\n",
      "loss:  2.3089723587036133\n",
      "loss:  2.290647268295288\n",
      "loss:  2.29666805267334\n",
      "loss:  2.294161319732666\n",
      "loss:  2.289747714996338\n",
      "loss:  2.3221168518066406\n",
      "loss:  2.3134403228759766\n",
      "loss:  2.3088371753692627\n",
      "loss:  2.3037424087524414\n",
      "loss:  2.2876980304718018\n",
      "loss:  2.2905030250549316\n",
      "loss:  2.3027474880218506\n",
      "loss:  2.2971081733703613\n",
      "loss:  2.3040060997009277\n",
      "loss:  2.3003928661346436\n",
      "loss:  2.311458110809326\n",
      "loss:  2.312695264816284\n",
      "loss:  2.3073904514312744\n",
      "loss:  2.3241708278656006\n",
      "loss:  2.2981984615325928\n",
      "loss:  2.3070130348205566\n",
      "loss:  2.3017096519470215\n",
      "loss:  2.310753107070923\n",
      "loss:  2.310218572616577\n",
      "loss:  2.301382303237915\n",
      "loss:  2.310588836669922\n",
      "loss:  2.2941858768463135\n",
      "loss:  2.312868356704712\n",
      "loss:  2.3013734817504883\n",
      "loss:  2.308826446533203\n",
      "loss:  2.305551767349243\n",
      "loss:  2.318098306655884\n",
      "loss:  2.300992012023926\n",
      "loss:  2.2948381900787354\n",
      "loss:  2.302952289581299\n",
      "loss:  2.3073995113372803\n",
      "loss:  2.325725793838501\n",
      "loss:  2.2857532501220703\n",
      "loss:  2.2979345321655273\n",
      "loss:  2.296154260635376\n",
      "loss:  2.3063204288482666\n",
      "loss:  2.3038952350616455\n",
      "loss:  2.3139424324035645\n",
      "loss:  2.302834987640381\n",
      "loss:  2.3179821968078613\n",
      "loss:  2.309497117996216\n",
      "loss:  2.336564779281616\n",
      "loss:  2.29990553855896\n",
      "loss:  2.30496883392334\n",
      "loss:  2.3059544563293457\n",
      "loss:  2.312568187713623\n",
      "loss:  2.320988178253174\n",
      "loss:  2.29866623878479\n",
      "loss:  2.3042426109313965\n",
      "loss:  2.329042434692383\n",
      "loss:  2.30376935005188\n",
      "loss:  2.3112332820892334\n",
      "loss:  2.292402982711792\n",
      "loss:  2.30771803855896\n",
      "loss:  2.2994446754455566\n",
      "loss:  2.295616388320923\n",
      "loss:  2.3068108558654785\n",
      "loss:  2.309382200241089\n",
      "loss:  2.3007097244262695\n",
      "loss:  2.290593147277832\n",
      "loss:  2.3082921504974365\n",
      "loss:  2.285522937774658\n",
      "loss:  2.301623582839966\n",
      "loss:  2.2997443675994873\n",
      "loss:  2.2997944355010986\n",
      "loss:  2.3088340759277344\n",
      "loss:  2.310739278793335\n",
      "loss:  2.3028717041015625\n",
      "loss:  2.3184707164764404\n",
      "loss:  2.3050363063812256\n",
      "loss:  2.310350179672241\n",
      "loss:  2.3103444576263428\n",
      "loss:  2.286611557006836\n",
      "loss:  2.2746188640594482\n",
      "loss:  2.3161323070526123\n",
      "loss:  2.2912166118621826\n",
      "loss:  2.2938015460968018\n",
      "loss:  2.293092966079712\n",
      "loss:  2.314140558242798\n",
      "loss:  2.3144097328186035\n",
      "loss:  2.289884090423584\n",
      "loss:  2.2873387336730957\n",
      "loss:  2.3069326877593994\n",
      "loss:  2.3220889568328857\n",
      "loss:  2.282524824142456\n",
      "loss:  2.2887048721313477\n",
      "loss:  2.297224521636963\n",
      "loss:  2.2877817153930664\n",
      "loss:  2.30973744392395\n",
      "loss:  2.2929069995880127\n",
      "loss:  2.3240575790405273\n",
      "loss:  2.3125691413879395\n",
      "loss:  2.307373046875\n",
      "loss:  2.3012607097625732\n",
      "loss:  2.318493366241455\n",
      "loss:  2.3133468627929688\n",
      "loss:  2.2971749305725098\n",
      "loss:  2.304213285446167\n",
      "loss:  2.302370548248291\n",
      "loss:  2.313323736190796\n",
      "loss:  2.2827682495117188\n",
      "loss:  2.286710500717163\n",
      "loss:  2.2989258766174316\n",
      "loss:  2.2972991466522217\n",
      "loss:  2.307150363922119\n",
      "loss:  2.307769536972046\n",
      "loss:  2.3232362270355225\n",
      "loss:  2.2918004989624023\n",
      "loss:  2.327584743499756\n",
      "loss:  2.2918877601623535\n",
      "loss:  2.303633213043213\n",
      "loss:  2.3246264457702637\n",
      "loss:  2.30009388923645\n",
      "loss:  2.294867992401123\n",
      "loss:  2.3195462226867676\n",
      "loss:  2.2829158306121826\n",
      "loss:  2.2961323261260986\n",
      "loss:  2.295226812362671\n",
      "loss:  2.2991435527801514\n",
      "loss:  2.2863011360168457\n",
      "loss:  2.2937769889831543\n",
      "loss:  2.309400796890259\n",
      "loss:  2.3362112045288086\n",
      "loss:  2.2875688076019287\n",
      "loss:  2.3143537044525146\n",
      "loss:  2.3171772956848145\n",
      "loss:  2.2963528633117676\n",
      "loss:  2.300025463104248\n",
      "loss:  2.283985137939453\n",
      "loss:  2.29799747467041\n",
      "loss:  2.308483839035034\n",
      "loss:  2.300567150115967\n",
      "loss:  2.309175968170166\n",
      "loss:  2.3206968307495117\n",
      "loss:  2.3082618713378906\n",
      "loss:  2.2966203689575195\n",
      "loss:  2.2949557304382324\n",
      "loss:  2.3043265342712402\n",
      "loss:  2.2993078231811523\n",
      "loss:  2.296069860458374\n",
      "loss:  2.318075180053711\n",
      "loss:  2.29729962348938\n",
      "loss:  2.300957441329956\n",
      "loss:  2.274324655532837\n",
      "loss:  2.305939197540283\n",
      "loss:  2.3128325939178467\n",
      "loss:  2.311535120010376\n",
      "loss:  2.3008780479431152\n",
      "loss:  2.307760238647461\n",
      "loss:  2.3082895278930664\n",
      "loss:  2.303122043609619\n",
      "loss:  2.2932188510894775\n",
      "loss:  2.304774522781372\n",
      "loss:  2.3147175312042236\n",
      "loss:  2.3196144104003906\n",
      "loss:  2.309643268585205\n",
      "loss:  2.297328472137451\n",
      "loss:  2.313115119934082\n",
      "loss:  2.2944788932800293\n",
      "loss:  2.2995245456695557\n",
      "loss:  2.304943799972534\n",
      "loss:  2.3164761066436768\n",
      "loss:  2.317828893661499\n",
      "loss:  2.2907841205596924\n",
      "loss:  2.305082321166992\n",
      "loss:  2.303954601287842\n",
      "loss:  2.3030471801757812\n",
      "loss:  2.327685832977295\n",
      "loss:  2.303521156311035\n",
      "loss:  2.3057057857513428\n",
      "loss:  2.3066771030426025\n",
      "loss:  2.2999989986419678\n",
      "loss:  2.2984390258789062\n",
      "loss:  2.313108444213867\n",
      "loss:  2.2921202182769775\n",
      "loss:  2.320728063583374\n",
      "loss:  2.2908475399017334\n",
      "loss:  2.3149373531341553\n",
      "loss:  2.3107218742370605\n",
      "loss:  2.3194942474365234\n",
      "loss:  2.293192148208618\n",
      "loss:  2.2937209606170654\n",
      "loss:  2.3112125396728516\n",
      "loss:  2.318347454071045\n",
      "loss:  2.2996270656585693\n",
      "loss:  2.3162789344787598\n",
      "loss:  2.278031587600708\n",
      "loss:  2.299164295196533\n",
      "loss:  2.2890241146087646\n",
      "loss:  2.299837112426758\n",
      "loss:  2.300452709197998\n",
      "loss:  2.293046236038208\n",
      "loss:  2.3270370960235596\n",
      "loss:  2.284081220626831\n",
      "loss:  2.3103718757629395\n",
      "loss:  2.31339955329895\n",
      "loss:  2.310453176498413\n",
      "loss:  2.31164288520813\n",
      "loss:  2.311077833175659\n",
      "loss:  2.2997019290924072\n",
      "loss:  2.2958824634552\n",
      "loss:  2.3003783226013184\n",
      "loss:  2.306417942047119\n",
      "loss:  2.3008954524993896\n",
      "loss:  2.2866923809051514\n",
      "loss:  2.3175711631774902\n",
      "loss:  2.305840015411377\n",
      "loss:  2.319594621658325\n",
      "loss:  2.3124122619628906\n",
      "loss:  2.296410083770752\n",
      "loss:  2.3046162128448486\n",
      "loss:  2.301553964614868\n",
      "loss:  2.310309886932373\n",
      "loss:  2.3150479793548584\n",
      "loss:  2.294167995452881\n",
      "loss:  2.2887048721313477\n",
      "loss:  2.307573080062866\n",
      "loss:  2.298942804336548\n",
      "loss:  2.314697265625\n",
      "loss:  2.2978553771972656\n",
      "loss:  2.3082408905029297\n",
      "loss:  2.296457290649414\n",
      "loss:  2.2942137718200684\n",
      "loss:  2.296419143676758\n",
      "loss:  2.2972912788391113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.311678886413574\n",
      "loss:  2.302957057952881\n",
      "loss:  2.2797884941101074\n",
      "loss:  2.3008992671966553\n",
      "loss:  2.3103549480438232\n",
      "loss:  2.291743755340576\n",
      "loss:  2.3051838874816895\n",
      "loss:  2.299081325531006\n",
      "loss:  2.319342613220215\n",
      "loss:  2.318666934967041\n",
      "loss:  2.2992939949035645\n",
      "loss:  2.2905871868133545\n",
      "loss:  2.3057048320770264\n",
      "loss:  2.306806802749634\n",
      "loss:  2.282799243927002\n",
      "loss:  2.2901997566223145\n",
      "loss:  2.3149967193603516\n",
      "loss:  2.319026231765747\n",
      "loss:  2.296133041381836\n",
      "loss:  2.323418140411377\n",
      "loss:  2.284257411956787\n",
      "loss:  2.293564796447754\n",
      "loss:  2.2871739864349365\n",
      "loss:  2.301591396331787\n",
      "loss:  2.314363479614258\n",
      "loss:  2.295255184173584\n",
      "loss:  2.290972948074341\n",
      "loss:  2.2932512760162354\n",
      "loss:  2.304779052734375\n",
      "loss:  2.3203847408294678\n",
      "loss:  2.301593542098999\n",
      "loss:  2.2887284755706787\n",
      "loss:  2.3084170818328857\n",
      "loss:  2.3123700618743896\n",
      "loss:  2.310504674911499\n",
      "loss:  2.3122525215148926\n",
      "loss:  2.279378652572632\n",
      "loss:  2.2749366760253906\n",
      "loss:  2.2935171127319336\n",
      "loss:  2.3339614868164062\n",
      "loss:  2.2945809364318848\n",
      "loss:  2.3063480854034424\n",
      "loss:  2.295689821243286\n",
      "loss:  2.3199868202209473\n",
      "loss:  2.3199524879455566\n",
      "loss:  2.2951018810272217\n",
      "loss:  2.3122124671936035\n",
      "loss:  2.3100826740264893\n",
      "loss:  2.2888877391815186\n",
      "loss:  2.3106577396392822\n",
      "loss:  2.2998294830322266\n",
      "loss:  2.2841105461120605\n",
      "loss:  2.300126314163208\n",
      "loss:  2.291963815689087\n",
      "loss:  2.2898905277252197\n",
      "loss:  2.3059189319610596\n",
      "loss:  2.3041272163391113\n",
      "loss:  2.3165369033813477\n",
      "loss:  2.3094918727874756\n",
      "loss:  2.316899061203003\n",
      "loss:  2.286698818206787\n",
      "loss:  2.282810688018799\n",
      "loss:  2.3129725456237793\n",
      "loss:  2.296189308166504\n",
      "loss:  2.294433116912842\n",
      "loss:  2.292207956314087\n",
      "loss:  2.276843309402466\n",
      "loss:  2.298942804336548\n",
      "loss:  2.2921652793884277\n",
      "loss:  2.3023288249969482\n",
      "loss:  2.2849559783935547\n",
      "loss:  2.304701566696167\n",
      "loss:  2.3002257347106934\n",
      "loss:  2.2983558177948\n",
      "loss:  2.307631015777588\n",
      "loss:  2.3086464405059814\n",
      "loss:  2.308465003967285\n",
      "loss:  2.29508376121521\n",
      "loss:  2.3033409118652344\n",
      "loss:  2.2999093532562256\n",
      "loss:  2.3227243423461914\n",
      "loss:  2.2842214107513428\n",
      "loss:  2.288515567779541\n",
      "loss:  2.2952427864074707\n",
      "loss:  2.2753891944885254\n",
      "loss:  2.301604747772217\n",
      "loss:  2.3184804916381836\n",
      "loss:  2.2976274490356445\n",
      "loss:  2.281320810317993\n",
      "loss:  2.283590316772461\n",
      "loss:  2.3038294315338135\n",
      "loss:  2.274120569229126\n",
      "loss:  2.302701950073242\n",
      "loss:  2.289454460144043\n",
      "loss:  2.3063666820526123\n",
      "loss:  2.2969882488250732\n",
      "loss:  2.3192384243011475\n",
      "loss:  2.3002960681915283\n",
      "loss:  2.284644603729248\n",
      "loss:  2.306696653366089\n",
      "loss:  2.3062217235565186\n",
      "loss:  2.2786598205566406\n",
      "loss:  2.2996952533721924\n",
      "loss:  2.288127899169922\n",
      "loss:  2.2944207191467285\n",
      "loss:  2.2773690223693848\n",
      "loss:  2.3121659755706787\n",
      "loss:  2.319429636001587\n",
      "loss:  2.30012583732605\n",
      "loss:  2.2893080711364746\n",
      "loss:  2.2966277599334717\n",
      "loss:  2.2879371643066406\n",
      "loss:  2.3001410961151123\n",
      "loss:  2.3070342540740967\n",
      "loss:  2.313230514526367\n",
      "loss:  2.3108861446380615\n",
      "loss:  2.2881786823272705\n",
      "loss:  2.296257495880127\n",
      "loss:  2.2924396991729736\n",
      "loss:  2.3011834621429443\n",
      "loss:  2.2939791679382324\n",
      "loss:  2.3232383728027344\n",
      "loss:  2.3152666091918945\n",
      "loss:  2.2935166358947754\n",
      "loss:  2.2942287921905518\n",
      "loss:  2.2817111015319824\n",
      "loss:  2.305323362350464\n",
      "loss:  2.2797749042510986\n",
      "loss:  2.308300018310547\n",
      "loss:  2.301558494567871\n",
      "loss:  2.2952935695648193\n",
      "loss:  2.3017160892486572\n",
      "loss:  2.291724681854248\n",
      "loss:  2.3005530834198\n",
      "loss:  2.3078479766845703\n",
      "loss:  2.302354574203491\n",
      "loss:  2.2849678993225098\n",
      "loss:  2.2902886867523193\n",
      "loss:  2.3103785514831543\n",
      "loss:  2.307638645172119\n",
      "loss:  2.2976064682006836\n",
      "loss:  2.3080625534057617\n",
      "loss:  2.2849652767181396\n",
      "loss:  2.299348831176758\n",
      "loss:  2.3052256107330322\n",
      "loss:  2.2755565643310547\n",
      "loss:  2.283069610595703\n",
      "loss:  2.2918145656585693\n",
      "loss:  2.3024656772613525\n",
      "loss:  2.290938138961792\n",
      "loss:  2.2882893085479736\n",
      "loss:  2.280784845352173\n",
      "loss:  2.2952747344970703\n",
      "loss:  2.2908413410186768\n",
      "loss:  2.2920923233032227\n",
      "loss:  2.306671619415283\n",
      "loss:  2.2855329513549805\n",
      "loss:  2.2944412231445312\n",
      "loss:  2.3032429218292236\n",
      "loss:  2.2944741249084473\n",
      "loss:  2.2907164096832275\n",
      "loss:  2.2863166332244873\n",
      "loss:  2.305279016494751\n",
      "loss:  2.3066694736480713\n",
      "loss:  2.288515567779541\n",
      "loss:  2.301809549331665\n",
      "loss:  2.2964046001434326\n",
      "loss:  2.314404010772705\n",
      "loss:  2.3188791275024414\n",
      "loss:  2.2837142944335938\n",
      "loss:  2.3061206340789795\n",
      "loss:  2.294541597366333\n",
      "loss:  2.286144733428955\n",
      "loss:  2.2977447509765625\n",
      "loss:  2.282386541366577\n",
      "loss:  2.2936596870422363\n",
      "loss:  2.315448522567749\n",
      "loss:  2.290283441543579\n",
      "loss:  2.306373119354248\n",
      "loss:  2.3019461631774902\n",
      "loss:  2.2869067192077637\n",
      "loss:  2.2952468395233154\n",
      "loss:  2.294649839401245\n",
      "loss:  2.283172845840454\n",
      "loss:  2.2801923751831055\n",
      "loss:  2.3011131286621094\n",
      "loss:  2.291193962097168\n",
      "loss:  2.2744486331939697\n",
      "loss:  2.2979562282562256\n",
      "loss:  2.282186508178711\n",
      "loss:  2.287233352661133\n",
      "loss:  2.3001904487609863\n",
      "loss:  2.280202865600586\n",
      "loss:  2.2870147228240967\n",
      "loss:  2.2887725830078125\n",
      "loss:  2.291332721710205\n",
      "loss:  2.295600414276123\n",
      "loss:  2.298480987548828\n",
      "loss:  2.2963390350341797\n",
      "loss:  2.298504114151001\n",
      "loss:  2.297719717025757\n",
      "loss:  2.288203716278076\n",
      "loss:  2.2808094024658203\n",
      "loss:  2.2950286865234375\n",
      "loss:  2.294189691543579\n",
      "loss:  2.2971882820129395\n",
      "loss:  2.2934954166412354\n",
      "loss:  2.297848701477051\n",
      "loss:  2.288149118423462\n",
      "loss:  2.2969558238983154\n",
      "loss:  2.275367498397827\n",
      "loss:  2.2786672115325928\n",
      "loss:  2.2915401458740234\n",
      "loss:  2.2832937240600586\n",
      "loss:  2.294870138168335\n",
      "loss:  2.27203369140625\n",
      "loss:  2.279484272003174\n",
      "loss:  2.2890727519989014\n",
      "loss:  2.289336681365967\n",
      "loss:  2.290585994720459\n",
      "loss:  2.2892842292785645\n",
      "loss:  2.287921905517578\n",
      "loss:  2.2930402755737305\n",
      "loss:  2.3032302856445312\n",
      "loss:  2.280515432357788\n",
      "loss:  2.2837767601013184\n",
      "loss:  2.2734851837158203\n",
      "loss:  2.2811150550842285\n",
      "loss:  2.303452968597412\n",
      "loss:  2.2838809490203857\n",
      "loss:  2.271022319793701\n",
      "loss:  2.2670695781707764\n",
      "loss:  2.2862603664398193\n",
      "loss:  2.2596988677978516\n",
      "loss:  2.263120174407959\n",
      "loss:  2.289935827255249\n",
      "loss:  2.288210153579712\n",
      "loss:  2.2599740028381348\n",
      "loss:  2.3023414611816406\n",
      "loss:  2.2935988903045654\n",
      "loss:  2.274745464324951\n",
      "loss:  2.2847440242767334\n",
      "loss:  2.265946626663208\n",
      "loss:  2.2827816009521484\n",
      "loss:  2.2703745365142822\n",
      "loss:  2.285799026489258\n",
      "loss:  2.275181770324707\n",
      "loss:  2.2817890644073486\n",
      "loss:  2.292032241821289\n",
      "loss:  2.2968180179595947\n",
      "loss:  2.2794315814971924\n",
      "loss:  2.2730185985565186\n",
      "loss:  2.2817277908325195\n",
      "loss:  2.2718348503112793\n",
      "loss:  2.2672510147094727\n",
      "loss:  2.27801513671875\n",
      "loss:  2.309134006500244\n",
      "loss:  2.2884345054626465\n",
      "loss:  2.2559263706207275\n",
      "loss:  2.293497323989868\n",
      "loss:  2.285461902618408\n",
      "loss:  2.2983341217041016\n",
      "loss:  2.2737741470336914\n",
      "loss:  2.263611316680908\n",
      "loss:  2.2713398933410645\n",
      "loss:  2.256481885910034\n",
      "loss:  2.271578550338745\n",
      "loss:  2.2606329917907715\n",
      "loss:  2.2696738243103027\n",
      "loss:  2.2673840522766113\n",
      "loss:  2.2653775215148926\n",
      "loss:  2.2863166332244873\n",
      "loss:  2.2367985248565674\n",
      "loss:  2.2566535472869873\n",
      "loss:  2.2797744274139404\n",
      "loss:  2.2861709594726562\n",
      "loss:  2.2422258853912354\n",
      "loss:  2.2486164569854736\n",
      "loss:  2.2677063941955566\n",
      "loss:  2.2551028728485107\n",
      "loss:  2.2421302795410156\n",
      "loss:  2.2544052600860596\n",
      "loss:  2.24483060836792\n",
      "loss:  2.2559146881103516\n",
      "loss:  2.2604737281799316\n",
      "loss:  2.2859270572662354\n",
      "loss:  2.2373721599578857\n",
      "loss:  2.2733192443847656\n",
      "loss:  2.2571966648101807\n",
      "loss:  2.2267751693725586\n",
      "loss:  2.2495903968811035\n",
      "loss:  2.238621473312378\n",
      "loss:  2.277933120727539\n",
      "loss:  2.22367525100708\n",
      "loss:  2.226516008377075\n",
      "loss:  2.246711492538452\n",
      "loss:  2.2478280067443848\n",
      "loss:  2.2419538497924805\n",
      "loss:  2.19579815864563\n",
      "loss:  2.196417808532715\n",
      "loss:  2.2347419261932373\n",
      "loss:  2.2467215061187744\n",
      "loss:  2.2081429958343506\n",
      "loss:  2.248548746109009\n",
      "loss:  2.189976930618286\n",
      "loss:  2.2120182514190674\n",
      "loss:  2.22406268119812\n",
      "loss:  2.1831161975860596\n",
      "loss:  2.1947238445281982\n",
      "loss:  2.1724729537963867\n",
      "loss:  2.216688394546509\n",
      "loss:  2.2286293506622314\n",
      "loss:  2.190114736557007\n",
      "loss:  2.1577303409576416\n",
      "loss:  2.1821658611297607\n",
      "loss:  2.1933400630950928\n",
      "loss:  2.130164623260498\n",
      "loss:  2.2326717376708984\n",
      "loss:  2.1679813861846924\n",
      "loss:  2.1357219219207764\n",
      "loss:  2.1357829570770264\n",
      "loss:  2.1083645820617676\n",
      "loss:  2.1088762283325195\n",
      "loss:  2.129653215408325\n",
      "loss:  2.1393826007843018\n",
      "loss:  2.1413238048553467\n",
      "loss:  2.056800127029419\n",
      "loss:  1.9780577421188354\n",
      "loss:  2.0723671913146973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.9619362354278564\n",
      "loss:  2.0531084537506104\n",
      "loss:  1.953859567642212\n",
      "loss:  1.9528391361236572\n",
      "loss:  2.068725347518921\n",
      "loss:  1.9576945304870605\n",
      "loss:  1.902992844581604\n",
      "loss:  1.9751232862472534\n",
      "loss:  1.8568426370620728\n",
      "loss:  1.8085826635360718\n",
      "loss:  1.6535122394561768\n",
      "loss:  1.622475028038025\n",
      "loss:  1.7765125036239624\n",
      "loss:  1.8534903526306152\n",
      "loss:  1.6979081630706787\n",
      "loss:  1.686481237411499\n",
      "loss:  1.6555129289627075\n",
      "loss:  1.4944467544555664\n",
      "loss:  1.5066384077072144\n",
      "loss:  1.7564483880996704\n",
      "loss:  1.374832272529602\n",
      "loss:  1.4050567150115967\n",
      "loss:  1.555950403213501\n",
      "loss:  1.3517922163009644\n",
      "loss:  1.6826801300048828\n",
      "loss:  1.3140567541122437\n",
      "loss:  1.3487836122512817\n",
      "loss:  1.3496381044387817\n",
      "loss:  1.452699899673462\n",
      "loss:  1.0873793363571167\n",
      "loss:  1.3923125267028809\n",
      "loss:  1.1990046501159668\n",
      "loss:  1.38873291015625\n",
      "loss:  1.3843834400177002\n",
      "loss:  1.7923566102981567\n",
      "loss:  1.5597927570343018\n",
      "loss:  1.480389952659607\n",
      "loss:  1.3415462970733643\n",
      "loss:  1.2000666856765747\n",
      "loss:  1.1855942010879517\n",
      "loss:  1.6476317644119263\n",
      "loss:  1.462000846862793\n",
      "loss:  1.2518577575683594\n",
      "loss:  1.149151086807251\n",
      "loss:  1.1863114833831787\n",
      "loss:  1.0625696182250977\n",
      "loss:  0.7823477983474731\n",
      "loss:  1.4597126245498657\n",
      "loss:  1.3616141080856323\n",
      "loss:  1.1836153268814087\n",
      "loss:  1.2056835889816284\n",
      "loss:  0.8175384998321533\n",
      "loss:  1.2392513751983643\n",
      "loss:  1.2267640829086304\n",
      "loss:  0.9450588226318359\n",
      "loss:  1.4963443279266357\n",
      "loss:  1.364503264427185\n",
      "loss:  1.0725929737091064\n",
      "loss:  1.3549315929412842\n",
      "loss:  1.369553804397583\n",
      "loss:  1.2199516296386719\n",
      "loss:  1.601400375366211\n",
      "loss:  1.1353495121002197\n",
      "loss:  1.65363609790802\n",
      "loss:  1.208813190460205\n",
      "loss:  1.0062427520751953\n",
      "loss:  0.7835729718208313\n",
      "loss:  1.0373510122299194\n",
      "loss:  1.0744845867156982\n",
      "loss:  0.9461425542831421\n",
      "loss:  1.6411633491516113\n",
      "loss:  1.585102915763855\n",
      "loss:  1.5939630270004272\n",
      "loss:  1.13015878200531\n",
      "loss:  1.1876791715621948\n",
      "loss:  0.9515900015830994\n",
      "loss:  0.932664155960083\n",
      "loss:  1.0536370277404785\n",
      "loss:  0.9627741575241089\n",
      "loss:  1.1935476064682007\n",
      "loss:  1.3609697818756104\n",
      "loss:  1.3011345863342285\n",
      "loss:  1.1572414636611938\n",
      "loss:  1.0488406419754028\n",
      "loss:  1.042467713356018\n",
      "loss:  1.0618940591812134\n",
      "loss:  0.7113980650901794\n",
      "loss:  1.14739990234375\n",
      "loss:  1.5010261535644531\n",
      "loss:  1.5401649475097656\n",
      "loss:  1.4734090566635132\n",
      "loss:  1.1352554559707642\n",
      "loss:  1.1841474771499634\n",
      "loss:  1.0226826667785645\n",
      "loss:  0.7812921404838562\n",
      "loss:  1.0301446914672852\n",
      "loss:  1.398665189743042\n",
      "loss:  1.1334222555160522\n",
      "loss:  0.976668119430542\n",
      "loss:  1.102741003036499\n",
      "loss:  1.0322257280349731\n",
      "loss:  1.211988925933838\n",
      "loss:  1.3657046556472778\n",
      "loss:  1.5982328653335571\n",
      "loss:  1.4292296171188354\n",
      "loss:  1.0829081535339355\n",
      "loss:  1.1816781759262085\n",
      "loss:  0.8731526136398315\n",
      "loss:  0.9498071074485779\n",
      "loss:  0.8680866956710815\n",
      "loss:  0.8932797908782959\n",
      "loss:  1.0843652486801147\n",
      "loss:  1.4487913846969604\n",
      "loss:  1.1555767059326172\n",
      "loss:  0.9678596258163452\n",
      "loss:  1.1906211376190186\n",
      "loss:  0.9047892689704895\n",
      "loss:  0.9008672833442688\n",
      "loss:  0.9222913980484009\n",
      "loss:  0.7192267775535583\n",
      "loss:  1.360343098640442\n",
      "loss:  1.0845469236373901\n",
      "loss:  0.8934692144393921\n",
      "loss:  0.7877840995788574\n",
      "loss:  0.9129917621612549\n",
      "loss:  0.7467911839485168\n",
      "loss:  1.0656862258911133\n",
      "loss:  0.7932030558586121\n",
      "loss:  1.2413712739944458\n",
      "loss:  1.5347096920013428\n",
      "loss:  1.2336761951446533\n",
      "loss:  1.2073348760604858\n",
      "loss:  0.7895848751068115\n",
      "loss:  0.9722274541854858\n",
      "loss:  0.7656819224357605\n",
      "loss:  1.1609503030776978\n",
      "loss:  1.4671603441238403\n",
      "loss:  1.0060571432113647\n",
      "loss:  1.0991055965423584\n",
      "loss:  1.6146507263183594\n",
      "loss:  1.1448092460632324\n",
      "loss:  1.016377329826355\n",
      "loss:  1.0578323602676392\n",
      "loss:  0.8766775727272034\n",
      "loss:  1.0414304733276367\n",
      "loss:  1.2909181118011475\n",
      "loss:  1.199524164199829\n",
      "loss:  1.497018575668335\n",
      "loss:  1.2281016111373901\n",
      "loss:  0.8954206109046936\n",
      "loss:  0.8483154773712158\n",
      "loss:  1.218720555305481\n",
      "loss:  0.9229075312614441\n",
      "loss:  1.1536415815353394\n",
      "loss:  0.9211721420288086\n",
      "loss:  0.8729579448699951\n",
      "loss:  0.8461957573890686\n",
      "loss:  0.6505022048950195\n",
      "loss:  1.0665949583053589\n",
      "loss:  0.5832898020744324\n",
      "loss:  0.6988326907157898\n",
      "loss:  1.1012765169143677\n",
      "loss:  0.7769508957862854\n",
      "loss:  1.330280065536499\n",
      "loss:  1.0196255445480347\n",
      "loss:  0.8848737478256226\n",
      "loss:  0.9730328917503357\n",
      "loss:  1.04367995262146\n",
      "loss:  1.1633540391921997\n",
      "loss:  0.7318270802497864\n",
      "loss:  1.0558576583862305\n",
      "loss:  0.9278349280357361\n",
      "loss:  1.1313514709472656\n",
      "loss:  0.9375668168067932\n",
      "loss:  0.8284788727760315\n",
      "loss:  0.6962416172027588\n",
      "loss:  0.7823516726493835\n",
      "loss:  1.2097194194793701\n",
      "loss:  0.8330599069595337\n",
      "loss:  1.0798249244689941\n",
      "loss:  1.1490947008132935\n",
      "loss:  1.3054691553115845\n",
      "loss:  0.9572450518608093\n",
      "loss:  0.9771848320960999\n",
      "loss:  0.9312421083450317\n",
      "loss:  1.0320017337799072\n",
      "loss:  1.2368154525756836\n",
      "loss:  1.251763939857483\n",
      "loss:  0.6750760674476624\n",
      "loss:  0.6745370030403137\n",
      "loss:  1.0989125967025757\n",
      "loss:  1.0718568563461304\n",
      "loss:  0.7983260154724121\n",
      "loss:  0.6871625185012817\n",
      "loss:  0.7092348337173462\n",
      "loss:  1.4471640586853027\n",
      "loss:  1.2689462900161743\n",
      "loss:  1.1161171197891235\n",
      "loss:  0.8534698486328125\n",
      "loss:  0.9252585768699646\n",
      "loss:  1.1662575006484985\n",
      "loss:  0.9321924448013306\n",
      "loss:  0.847827136516571\n",
      "loss:  0.3650907874107361\n",
      "loss:  0.6992207169532776\n",
      "loss:  1.2103509902954102\n",
      "loss:  2.159912109375\n",
      "loss:  2.108405590057373\n",
      "loss:  1.774737000465393\n",
      "loss:  1.157102108001709\n",
      "loss:  1.8343603610992432\n",
      "loss:  1.8490389585494995\n",
      "loss:  1.5206265449523926\n",
      "loss:  1.000986933708191\n",
      "loss:  2.0822315216064453\n",
      "loss:  1.9056999683380127\n",
      "loss:  1.349841833114624\n",
      "loss:  1.1801024675369263\n",
      "loss:  1.2382341623306274\n",
      "loss:  1.1795060634613037\n",
      "loss:  1.3200081586837769\n",
      "loss:  0.8954066634178162\n",
      "loss:  0.6712956428527832\n",
      "loss:  0.9318727254867554\n",
      "loss:  1.1967885494232178\n",
      "loss:  3.3064799308776855\n",
      "loss:  2.14237642288208\n",
      "loss:  2.01798152923584\n",
      "loss:  1.6852991580963135\n",
      "loss:  1.7251843214035034\n",
      "loss:  2.113168239593506\n",
      "loss:  1.8004153966903687\n",
      "loss:  1.370794415473938\n",
      "loss:  1.6202008724212646\n",
      "loss:  1.8914830684661865\n",
      "loss:  1.511484146118164\n",
      "loss:  1.692704200744629\n",
      "loss:  1.9227828979492188\n",
      "loss:  1.8718857765197754\n",
      "loss:  1.671997308731079\n",
      "loss:  0.9762781262397766\n",
      "loss:  1.0936930179595947\n",
      "loss:  1.9340720176696777\n",
      "loss:  2.2171313762664795\n",
      "loss:  1.864001750946045\n",
      "loss:  2.366302490234375\n",
      "loss:  2.0117552280426025\n",
      "loss:  1.897450566291809\n",
      "loss:  1.512646198272705\n",
      "loss:  1.1079661846160889\n",
      "loss:  1.5085543394088745\n",
      "loss:  2.4059412479400635\n",
      "loss:  2.360302209854126\n",
      "loss:  1.7734131813049316\n",
      "loss:  1.751551628112793\n",
      "loss:  1.8459981679916382\n",
      "loss:  1.5176383256912231\n",
      "loss:  1.616758108139038\n",
      "loss:  2.08899188041687\n",
      "loss:  1.8995721340179443\n",
      "loss:  1.4237887859344482\n",
      "loss:  1.4524470567703247\n",
      "loss:  1.6031183004379272\n",
      "loss:  1.1601243019104004\n",
      "loss:  1.3229186534881592\n",
      "loss:  1.923232913017273\n",
      "loss:  2.1623966693878174\n",
      "loss:  1.525469183921814\n",
      "loss:  2.7263050079345703\n",
      "loss:  1.803220510482788\n",
      "loss:  2.4713294506073\n",
      "loss:  1.9237340688705444\n",
      "loss:  2.050471782684326\n",
      "loss:  1.987006664276123\n",
      "loss:  1.4678701162338257\n",
      "loss:  1.7089513540267944\n",
      "loss:  2.049677610397339\n",
      "loss:  1.9324852228164673\n",
      "loss:  2.0248818397521973\n",
      "loss:  1.99637770652771\n",
      "loss:  1.954517126083374\n",
      "loss:  1.978590726852417\n",
      "loss:  1.9782108068466187\n",
      "loss:  1.612370252609253\n",
      "loss:  1.201047420501709\n",
      "loss:  2.624664306640625\n",
      "loss:  2.0004806518554688\n",
      "loss:  2.723865509033203\n",
      "loss:  2.296889543533325\n",
      "loss:  2.7303779125213623\n",
      "loss:  9.284005165100098\n",
      "loss:  2.5668253898620605\n",
      "loss:  2.4605979919433594\n",
      "loss:  1438.245849609375\n"
     ]
    }
   ],
   "source": [
    "# try multiple times (3-5 times) to get a good intuition and correctness\n",
    "model_lrfinder = FashionMnistNet()\n",
    "bs = 32\n",
    "loss_func = F.cross_entropy\n",
    "log_lrs, losses = find_lr(model_lrfinder, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11c650048>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([10**x for x in log_lrs], losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Augmentation**\n",
    "\n",
    "Some canonical transforms:\n",
    "- Cropping image randomly\n",
    "- Rotation\n",
    "\n",
    "*You can use PyTorch's in-built transforms module*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch provides many methods for data augmentation\n",
    "# useful to understand data wrappers and data loaders\n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "class FashionMnistDataset(Dataset):\n",
    "    \"Dataset class for Fashion Mnist data\"\n",
    "    def __init__(self, data_x, label_y, transforms=None):\n",
    "        self.data_x = data_x\n",
    "        self.label_y = label_y\n",
    "        self.transforms=transforms\n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data_x[index]\n",
    "        y = self.label_y[index]\n",
    "        return self.transforms(x.view(1, 28, 28)), y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(0.3),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "dataset = FashionMnistDataset(x_train, y_train, transform)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** [Batch normalization](https://www.youtube.com/watch?v=tNIpEZLv_eg) **\n",
    "\n",
    "** [Reasons why batchnorm work are still being debated](https://twitter.com/dcpage3/status/1171867587417952260) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** [Transfer Learning](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Chapter including history of deep learning](https://www.deeplearningbook.org/contents/intro.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some resources for free GPUs (limited use but good enough for personal projects)\n",
    "[Google Colab](https://research.google.com/colaboratory/faq.html)\n",
    "\n",
    "[Paperspace](https://gradient.paperspace.com/free-gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Credits for images and gifs](https://me.me/i/tweaking-neural-net-parameter-mecenter-com-none-20844606)\n",
    "\n",
    "[](https://engmrk.com/lenet-5-a-classic-cnn-architecture/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/bin/bash: run.sh: command not found']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}


# In[ ]:


def convNN(model, ch1, ch2, kernel, pool, nDense, drop, dropDense):
    
    model = tf.keras.models.Sequential() # create model                
    
    # first CONV => RELU => CONV => RELU => POOL layer set
    model.add(Conv2D(ch1, kernel, padding="same", 
                     activation="relu", input_shape=input_shape))
    model.add(BatchNormalization())
    model.add(Conv2D(ch1, kernel, padding="same", activation="relu"))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=pool))
    model.add(tf.keras.layers.Dropout(drop))
 
    # second CONV => RELU => CONV => RELU => POOL layer set
    model.add(Conv2D(ch2, kernel, padding="same", 
                     activation="relu", input_shape=input_shape))
    model.add(BatchNormalization())
    model.add(Conv2D(ch2, kernel, padding="same", activation="relu"))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=pool))
    model.add(tf.keras.layers.Dropout(drop))
 
    model.add(tf.keras.layers.Flatten()) 
    
    # FC => RELU layers
    model.add(tf.keras.layers.Dense(nDense, activation='relu'))
    model.add(BatchNormalization())
    model.add(tf.keras.layers.Dropout(dropDense))
    
    # output softmax layer
    model.add(tf.keras.layers.Dense(nClasses, activation='softmax'))
    
    model.compile(loss=tf.keras.losses.categorical_crossentropy,
                  optimizer=tf.keras.optimizers.Adadelta(),
                  metrics=['accuracy'])
 
    return model


# In[ ]:


def plotgraph(model, history):
    # plot history for accuracy
    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:




